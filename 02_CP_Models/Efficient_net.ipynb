{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOCTxJS5_fM9"
   },
   "outputs": [],
   "source": [
    "# 10 selected MoAs \n",
    "moas_to_use = ['Aurora kinase inhibitor', 'tubulin polymerization inhibitor', 'JAK inhibitor', 'protein synthesis inhibitor', 'HDAC inhibitor', \n",
    "        'topoisomerase inhibitor', 'PARP inhibitor', 'ATPase inhibitor', 'retinoid receptor agonist', 'HSP inhibitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lon5FqMB3JXq"
   },
   "outputs": [],
   "source": [
    "# read the data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "all_data = pd.read_csv('../data_for_image_based_model/fl_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the lines with DMSO \n",
    "all_data = all_data[all_data.moa != 'dmso']\n",
    "assert 'dmso' not in all_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'ATPase inhibitor': 7, 'Aurora kinase inhibitor': 0,\n",
    " 'HDAC inhibitor': 4, 'HSP inhibitor': 9, 'JAK inhibitor': 2, 'PARP inhibitor': 6,\n",
    " 'protein synthesis inhibitor': 3, 'retinoid receptor agonist': 8,\n",
    " 'topoisomerase inhibitor': 5, 'tubulin polymerization inhibitor': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change moa to classes \n",
    "all_data['classes'] = None\n",
    "for i in range(all_data.shape[0]):\n",
    "    all_data.iloc[i, 11] = dictionary[all_data.iloc[i, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'digit' column as index \n",
    "all_data['digit'] = [i for i in range(0, all_data.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data \n",
    "all_data = all_data.sample(frac = 1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the compounds in test data  \n",
    "compound_test =  ['CBK303888', 'CBK200949', 'CBK309016', 'CBK290799', 'CBK308876', 'CBK289882',\n",
    " 'CBK308108', 'CBK290852', 'CBK307956', 'CBK290529', 'CBK309488', 'CBK290717', 'CBK277957', 'CBK290547',\n",
    " 'CBK291084', 'CBK289740', 'CBK278064', 'CBK308819', 'CBK308608', 'CBK288281', 'CBK309437', 'CBK289987',\n",
    " 'CBK278556G', 'CBK277961']\n",
    "assert len(compound_test) == 24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the compounds in validation data \n",
    "compound_valid = ['CBK309483', 'CBK200593', 'CBK278120', 'CBK309545', 'CBK277955', 'CBK278047', 'CBK278016', 'CBK290481', 'CBK290217',\n",
    " 'CBK041182', 'CBK271643', 'CBK041804', 'CBK042154', 'CBK308980', 'CBK290803', 'CBK309258', 'CBK290154',\n",
    " 'CBK308260', 'CBK017131', 'CBK290770', 'CBK291055', 'CBK307747', 'CBK228145']\n",
    "assert len(compound_valid) == 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the compounds in training data \n",
    "compound_train = []\n",
    "for i in all_data.compound.tolist():\n",
    "    if i not in (compound_test + compound_valid + compound_train):\n",
    "        compound_train.append(i)\n",
    "\n",
    "assert len(compound_train) + len(compound_valid) + len(compound_test) == len(set(all_data.compound.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73_IwUtfO7sm"
   },
   "outputs": [],
   "source": [
    "# get the train, valid and test set \n",
    "train = all_data[all_data['compound'].isin(compound_train)].reset_index(drop = True)  \n",
    "valid = all_data[all_data['compound'].isin(compound_valid)].reset_index(drop = True)  \n",
    "test = all_data[all_data['compound'].isin(compound_test)].reset_index(drop = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpaHDV_NyUTz"
   },
   "outputs": [],
   "source": [
    "# get the dictionary for compound_id-SMILES pair \n",
    "import pickle\n",
    "compound_smiles_dictionary = pickle.load(open(\"../data_for_image_based_model/dictionary2.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDGCruBJA1J2"
   },
   "outputs": [],
   "source": [
    "# on the fly data augmentation \n",
    "import albumentations as A\n",
    "train_transforms = A.Compose([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.2),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.4),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.5),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.6),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.8),\n",
    "    A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),])\n",
    "valid_transforms = A.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDZ2gf4WCxcW"
   },
   "outputs": [],
   "source": [
    "# get all images  \n",
    "import numpy as np\n",
    "all_images = np.load(open(, \"rb\")) # The path has to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOHasY7qCrm4"
   },
   "outputs": [],
   "source": [
    "# data generator for training \n",
    "def get_train_image(end):\n",
    "  start = 0\n",
    "  while start < end:      \n",
    "    idx = start       \n",
    "    row = train.iloc[idx]\n",
    "\n",
    "    assert row['digit']     in train.digit.tolist()\n",
    "    assert row['compound']   in train.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in valid.digit.tolist()\n",
    "    assert row['compound']  not in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in test.digit.tolist()\n",
    "    assert row['compound']  not in test.compound.tolist()\n",
    "\n",
    "    image = all_images[all_data.digit.tolist().index(row['digit'])]  \n",
    "    image = train_transforms(image = image)['image']               \n",
    "    target = int(row['classes'])   \n",
    "    \n",
    "    yield image, target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbPwaU-dCro5"
   },
   "outputs": [],
   "source": [
    "# data generator for validation   \n",
    "def get_valid_image(end):\n",
    "  start = 0\n",
    "  while start<end:\n",
    "    idx = start       \n",
    "    row = valid.iloc[idx]\n",
    "\n",
    "    assert row['digit']   not in train.digit.tolist()\n",
    "    assert row['compound']  not in train.compound.tolist()\n",
    "\n",
    "    assert row['digit']     in valid.digit.tolist()\n",
    "    assert row['compound']   in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in test.digit.tolist()\n",
    "    assert row['compound']  not in test.compound.tolist()\n",
    "       \n",
    "    image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "    image = valid_transforms(image = image)['image']\n",
    "    target = int(row['classes'])   \n",
    "    \n",
    "    yield image, target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mz8y7LJCrrG"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "channels = 5      \n",
    "image_size = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6agIQbDGkw3"
   },
   "outputs": [],
   "source": [
    "# turn to tensorflow datasets \n",
    "import tensorflow as tf\n",
    "train_data = tf.data.Dataset.from_generator(get_train_image,\n",
    "            (tf.float32, tf.int32),\n",
    "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
    "            args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)\n",
    "\n",
    "valid_data = tf.data.Dataset.from_generator(get_valid_image,\n",
    "            (tf.float32, tf.int32),\n",
    "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
    "            args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPiZ1G_oG2Rl"
   },
   "outputs": [],
   "source": [
    "# we choose efficientnet b1 as the base model    \n",
    "base_model = tf.keras.applications.EfficientNetB1(input_shape = (image_size, image_size, channels),\n",
    "                                  include_top = False, weights = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-FX8pid1h_U"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbRdHCZx1h_U"
   },
   "outputs": [],
   "source": [
    "# complete the architecture of efficientnet b1\n",
    "drop = 0.30\n",
    "num_classes = len(set(train['classes'].tolist())) \n",
    "x = base_model.output\n",
    "x = Dropout(drop)(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = Dropout(drop)(x)\n",
    "preds = layers.Dense(num_classes, activation = 'softmax',                     \n",
    "    kernel_regularizer = regularizers.L1L2(l1 = 1e-4, l2 = 1e-3),\n",
    "    bias_regularizer = regularizers.L2(1e-3),\n",
    "    activity_regularizer = regularizers.L2(1e-4))(x)\n",
    "cnn_model = models.Model(inputs = base_model.input, outputs = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd4PkWWJ1h_V"
   },
   "outputs": [],
   "source": [
    "# set the optimizer of efficientnet b1    \n",
    "cnn_optimizer = tfa.optimizers.AdamW(weight_decay = 1e-6, learning_rate = 0.001, beta_1 = 0.9,\n",
    "    beta_2 = 0.999, epsilon = 1e-07,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLZSv4icG6Qv"
   },
   "outputs": [],
   "source": [
    "# compile the model   \n",
    "cnn_model.compile(optimizer = cnn_optimizer,\n",
    "         loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "         metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2s1BvcYG--W"
   },
   "outputs": [],
   "source": [
    "# set the class weights \n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', \n",
    "         classes = np.unique(train.classes), y = train.classes)   \n",
    "weight_dictionary = dict(zip(np.unique(train.classes), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buV4eg24HERe"
   },
   "outputs": [],
   "source": [
    "# set the check point   \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath_cnn = './content/shuffle_5_' + str(base_model.name) + '_weights.hdf5'\n",
    "checkpoint_cnn = ModelCheckpoint(filepath_cnn, monitor = 'val_accuracy', verbose = 0, \n",
    "                  save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecAtvKNaHVec"
   },
   "outputs": [],
   "source": [
    "# train the efficientnet b1\n",
    "from tensorflow.keras.callbacks import EarlyStopping  \n",
    "reduce_lr_loss_cnn = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
    "              factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
    "history_cnn = cnn_model.fit(train_data, validation_data = valid_data, class_weight = weight_dictionary,\n",
    "               verbose = 2, epochs = 100, callbacks=[reduce_lr_loss_cnn, checkpoint_cnn,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATgS1vCJp9ko"
   },
   "outputs": [],
   "source": [
    "# get the best model \n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "best_cnn_model = load_model() # Path has to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPopQovGLhYp"
   },
   "outputs": [],
   "source": [
    "# evaluate the model   \n",
    "predicted_test = []\n",
    "for i in range(test.shape[0]):    \n",
    "  row = test.iloc[i]\n",
    "\n",
    "  assert row['digit']  not  in train.digit.tolist()\n",
    "  assert row['compound'] not  in train.compound.tolist()\n",
    "  \n",
    "  assert row['digit']   not  in valid.digit.tolist()\n",
    "  assert row['compound']  not  in valid.compound.tolist()\n",
    "\n",
    "  assert row['digit']     in test.digit.tolist()\n",
    "  assert row['compound']    in test.compound.tolist()  \n",
    "\n",
    "  im = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "  im = valid_transforms(image = im)['image']\n",
    "  im = np.expand_dims(im, 0)\n",
    "  value = best_cnn_model.predict(im).argmax()\n",
    "  \n",
    "  predicted_test.append(value)   \n",
    "  \n",
    "print(classification_report(test.classes.tolist(), predicted_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kw9fcxklW2Y"
   },
   "outputs": [],
   "source": [
    "# references  \n",
    "# https://www.tensorflow.org/guide/data\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "# https://www.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
