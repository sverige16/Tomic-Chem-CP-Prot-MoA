{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omxK7KFWA7KJ"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/XinhaoLi74/SmilesPE/master/SPE_ChEMBL.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQRrK3yz37rM"
      },
      "outputs": [],
      "source": [
        "# Tokenizaion classes for huggingface interface\n",
        "# reference: https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py\n",
        "# reference https://github.com/rxn4chemistry/rxnmapper\n",
        "\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "import unicodedata\n",
        "import transformers\n",
        "from typing import List, Optional\n",
        "from transformers import PreTrainedTokenizer\n",
        "from SmilesPE.tokenizer import *\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        tokens = reader.readlines()\n",
        "    for index, token in enumerate(tokens):\n",
        "        token = token.rstrip(\"\\n\")\n",
        "        vocab[token] = index\n",
        "    return vocab\n",
        "\n",
        "class Atomwise_Tokenizer(object):\n",
        "    \"\"\"Run atom-level SMILES tokenization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Constructs a atom-level Tokenizer.\n",
        "        \"\"\"\n",
        "        self.regex_pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.regex_pattern)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\" Basic Tokenization of a SMILES.\n",
        "        \"\"\"\n",
        "        tokens = [token for token in self.regex.findall(text)]\n",
        "        return tokens\n",
        "    \n",
        "class SMILES_SPE_Tokenizer(PreTrainedTokenizer):\n",
        "    r\"\"\"\n",
        "    Constructs a SMILES tokenizer. Based on SMILES Pair Encoding (https://github.com/XinhaoLi74/SmilesPE).\n",
        "    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the methods. Users\n",
        "    should refer to the superclass for more information regarding methods.\n",
        "    Args:\n",
        "        vocab_file (:obj:`string`):\n",
        "            File containing the vocabulary.\n",
        "        spe_file (:obj:`string`):\n",
        "            File containing the trained SMILES Pair Encoding vocabulary.\n",
        "        unk_token (:obj:`string`, `optional`, defaults to \"[UNK]\"):\n",
        "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
        "            token instead.\n",
        "        sep_token (:obj:`string`, `optional`, defaults to \"[SEP]\"):\n",
        "            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n",
        "            for sequence classification or for a text and a question for question answering.\n",
        "            It is also used as the last token of a sequence built with special tokens.\n",
        "        pad_token (:obj:`string`, `optional`, defaults to \"[PAD]\"):\n",
        "            The token used for padding, for example when batching sequences of different lengths.\n",
        "        cls_token (:obj:`string`, `optional`, defaults to \"[CLS]\"):\n",
        "            The classifier token which is used when doing sequence classification (classification of the whole\n",
        "            sequence instead of per-token classification). It is the first token of the sequence when built with\n",
        "            special tokens.\n",
        "        mask_token (:obj:`string`, `optional`, defaults to \"[MASK]\"):\n",
        "            The token used for masking values. This is the token used when training this model with masked language\n",
        "            modeling. This is the token which the model will try to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        spe_file,\n",
        "        unk_token=\"[UNK]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'.\".format(vocab_file)\n",
        "            )\n",
        "        if not os.path.isfile(spe_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a SPE vocabulary file at path '{}'.\".format(spe_file)\n",
        "            )\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.spe_vocab = codecs.open(spe_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.spe_tokenizer = SPE_Tokenizer(self.spe_vocab)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.vocab, **self.added_tokens_encoder)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return self.spe_tokenizer.tokenize(text).split(' ')\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        return self.ids_to_tokens.get(index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
        "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A BERT sequence has the following format:\n",
        "        - single sequence: ``[CLS] X [SEP]``\n",
        "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of IDs to which the special tokens will be added\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "        Returns:\n",
        "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "                Set to True if the token list is already formatted with special tokens for the model\n",
        "        Returns:\n",
        "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "        ::\n",
        "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
        "            | first sequence    | second sequence |\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
        "            sequence(s).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, vocab_path):\n",
        "        \"\"\"\n",
        "        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n",
        "        Args:\n",
        "            vocab_path (:obj:`str`):\n",
        "                The directory in which to save the vocabulary.\n",
        "        Returns:\n",
        "            :obj:`Tuple(str)`: Paths to the files saved.\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        if os.path.isdir(vocab_path):\n",
        "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "        else:\n",
        "            vocab_file = vocab_path\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "        return (vocab_file,)\n",
        "\n",
        "class Atomwise_Tokenizer(object):\n",
        "    \"\"\"Run atom-level SMILES tokenization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Constructs a atom-level Tokenizer.\n",
        "        \"\"\"\n",
        "        self.regex_pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "        self.regex = re.compile(self.regex_pattern)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\" Basic Tokenization of a SMILES.\n",
        "        \"\"\"\n",
        "        tokens = [token for token in self.regex.findall(text)]\n",
        "        return tokens\n",
        "\n",
        "class SMILES_Atomwise_Tokenizer(PreTrainedTokenizer):\n",
        "    r\"\"\"\n",
        "    Constructs a SMILES tokenizer. Based on SMILES Pair Encoding (https://github.com/XinhaoLi74/SmilesPE).\n",
        "    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the methods. Users\n",
        "    should refer to the superclass for more information regarding methods.\n",
        "    Args:\n",
        "        vocab_file (:obj:`string`):\n",
        "            File containing the vocabulary.\n",
        "        unk_token (:obj:`string`, `optional`, defaults to \"[UNK]\"):\n",
        "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
        "            token instead.\n",
        "        sep_token (:obj:`string`, `optional`, defaults to \"[SEP]\"):\n",
        "            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n",
        "            for sequence classification or for a text and a question for question answering.\n",
        "            It is also used as the last token of a sequence built with special tokens.\n",
        "        pad_token (:obj:`string`, `optional`, defaults to \"[PAD]\"):\n",
        "            The token used for padding, for example when batching sequences of different lengths.\n",
        "        cls_token (:obj:`string`, `optional`, defaults to \"[CLS]\"):\n",
        "            The classifier token which is used when doing sequence classification (classification of the whole\n",
        "            sequence instead of per-token classification). It is the first token of the sequence when built with\n",
        "            special tokens.\n",
        "        mask_token (:obj:`string`, `optional`, defaults to \"[MASK]\"):\n",
        "            The token used for masking values. This is the token used when training this model with masked language\n",
        "            modeling. This is the token which the model will try to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        unk_token=\"[UNK]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'.\".format(vocab_file)\n",
        "            )\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.tokenizer = Atomwise_Tokenizer()\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.vocab, **self.added_tokens_encoder)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return self.tokenizer.tokenize(text)\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        return self.ids_to_tokens.get(index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
        "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A BERT sequence has the following format:\n",
        "        - single sequence: ``[CLS] X [SEP]``\n",
        "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of IDs to which the special tokens will be added\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "        Returns:\n",
        "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "                Set to True if the token list is already formatted with special tokens for the model\n",
        "        Returns:\n",
        "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "        ::\n",
        "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
        "            | first sequence    | second sequence |\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
        "            sequence(s).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, vocab_path):\n",
        "        \"\"\"\n",
        "        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n",
        "        Args:\n",
        "            vocab_path (:obj:`str`):\n",
        "                The directory in which to save the vocabulary.\n",
        "        Returns:\n",
        "            :obj:`Tuple(str)`: Paths to the files saved.\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        if os.path.isdir(vocab_path):\n",
        "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "        else:\n",
        "            vocab_file = vocab_path\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "        return (vocab_file,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOJu2M_Y37wt"
      },
      "outputs": [],
      "source": [
        "# some default tokens from huggingface\n",
        "default_toks = ['[PAD]', \n",
        "                '[unused1]', '[unused2]', '[unused3]', '[unused4]','[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', \n",
        "                '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
        "\n",
        "# atom-level tokens used for trained the spe vocabulary\n",
        "atom_toks = ['[c-]', '[SeH]', '[N]', '[C@@]', '[Te]', '[OH+]', 'n', '[AsH]', '[B]', 'b', \n",
        "             '[S@@]', 'o', ')', '[NH+]', '[SH]', 'O', 'I', '[C@]', '-', '[As+]', '[Cl+2]', \n",
        "             '[P+]', '[o+]', '[C]', '[C@H]', '[CH2]', '\\\\', 'P', '[O-]', '[NH-]', '[S@@+]', \n",
        "             '[te]', '[s+]', 's', '[B-]', 'B', 'F', '=', '[te+]', '[H]', '[C@@H]', '[Na]', \n",
        "             '[Si]', '[CH2-]', '[S@+]', 'C', '[se+]', '[cH-]', '6', 'N', '[IH2]', '[As]', \n",
        "             '[Si@]', '[BH3-]', '[Se]', 'Br', '[C+]', '[I+3]', '[b-]', '[P@+]', '[SH2]', '[I+2]', \n",
        "             '%11', '[Ag-3]', '[O]', '9', 'c', '[N-]', '[BH-]', '4', '[N@+]', '[SiH]', '[Cl+3]', '#', \n",
        "             '(', '[O+]', '[S-]', '[Br+2]', '[nH]', '[N+]', '[n-]', '3', '[Se+]', '[P@@]', '[Zn]', '2', \n",
        "             '[NH2+]', '%10', '[SiH2]', '[nH+]', '[Si@@]', '[P@@+]', '/', '1', '[c+]', '[S@]', '[S+]', \n",
        "             '[SH+]', '[B@@-]', '8', '[B@-]', '[C-]', '7', '[P@]', '[se]', 'S', '[n+]', '[PH]', '[I+]', '5', 'p', '[BH2-]', '[N@@+]', '[CH]', 'Cl']\n",
        "\n",
        "# spe tokens\n",
        "with open('SPE_ChEMBL.txt', \"r\") as ins:\n",
        "    spe_toks = []\n",
        "    for line in ins:\n",
        "        spe_toks.append(line.split('\\n')[0])\n",
        "\n",
        "spe_tokens = []\n",
        "for s in spe_toks:\n",
        "    spe_tokens.append(''.join(s.split(' ')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjp3seBZ374e"
      },
      "outputs": [],
      "source": [
        "spe_vocab = default_toks + atom_toks + spe_tokens\n",
        "with open('vocab_spe.txt', 'w') as f:\n",
        "  for voc in spe_vocab:\n",
        "    f.write(f'{voc}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJUsA6pP377D"
      },
      "outputs": [],
      "source": [
        "# set the tokenizer \n",
        "tokenizer = SMILES_SPE_Tokenizer(vocab_file = 'vocab_spe.txt', spe_file = 'SPE_ChEMBL.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4cC0Iy_TFpT"
      },
      "outputs": [],
      "source": [
        "# read the SMILES-MoA data  \n",
        "import pandas as pd\n",
        "df = pd.read_csv('top_20_MOAs.txt', sep = '\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn98U53k9qGt"
      },
      "outputs": [],
      "source": [
        "# change 20 MoAs to classes \n",
        "MOA_class_dictionary = {'EGFR inhibitor': 8,\n",
        " 'HDAC inhibitor': 16,\n",
        " 'PI3K inhibitor': 13,\n",
        " 'acetylcholine receptor agonist': 1,\n",
        " 'acetylcholine receptor antagonist': 4,\n",
        " 'adrenergic receptor agonist': 18,\n",
        " 'adrenergic receptor antagonist': 15,\n",
        " 'bacterial cell wall synthesis inhibitor': 14,\n",
        " 'benzodiazepine receptor agonist': 10,\n",
        " 'calcium channel blocker': 5,\n",
        " 'cyclooxygenase inhibitor': 6,\n",
        " 'dopamine receptor antagonist': 12,\n",
        " 'glucocorticoid receptor agonist': 9,\n",
        " 'glutamate receptor antagonist': 19,\n",
        " 'histamine receptor antagonist': 17,\n",
        " 'phosphodiesterase inhibitor': 3,\n",
        " 'serotonin receptor agonist': 7,\n",
        " 'serotonin receptor antagonist': 2,\n",
        " 'sodium channel blocker': 11,\n",
        " 'topoisomerase inhibitor': 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FBAtW0JH0ce"
      },
      "outputs": [],
      "source": [
        "# add classes column \n",
        "df['classes'] = None\n",
        "for i in range(df.shape[0]):\n",
        "  df.iloc[i,2] = MOA_class_dictionary[df.iloc[i,1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNPAfKr6JI18"
      },
      "outputs": [],
      "source": [
        "# Split out the test set  \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train_valid, x_test, y_train_valid, y_test = train_test_split(df.SMILES, df.classes, test_size =10/100,\n",
        " stratify = df.classes, shuffle = True, random_state = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47xiERRvJI65"
      },
      "outputs": [],
      "source": [
        "# kfold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "skf = StratifiedKFold(n_splits = 9)\n",
        "skf.get_n_splits(np.array(list(x_train_valid)), np.array(list(y_train_valid)))\n",
        "train_index_list = []\n",
        "valid_index_list = []\n",
        "for train_index, valid_index in skf.split(np.array(list(x_train_valid)), np.array(list(y_train_valid))):\n",
        "  train_index_list.append(train_index)\n",
        "  valid_index_list.append(valid_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKw3B-kfJWeY"
      },
      "outputs": [],
      "source": [
        "number_of_kfold = 0 # change the number from 0-8 to get 9 shuffles\n",
        "x_train = list(np.array(list(x_train_valid))[train_index_list[ number_of_kfold ]])\n",
        "x_valid = list(np.array(list(x_train_valid))[valid_index_list[ number_of_kfold ]])\n",
        "y_train = list(np.array(list(y_train_valid))[train_index_list[ number_of_kfold ]])\n",
        "y_valid = list(np.array(list(y_train_valid))[valid_index_list[ number_of_kfold ]])\n",
        "x_test = list(x_test)\n",
        "y_test = list(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTDbXH1SJWg8"
      },
      "outputs": [],
      "source": [
        "# turn to cannoical  smiles\n",
        "import rdkit\n",
        "import numpy as np\n",
        "from rdkit import *\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "x_train = [Chem.MolToSmiles(Chem.MolFromSmiles(smi),True) for smi in x_train]\n",
        "x_valid = [Chem.MolToSmiles(Chem.MolFromSmiles(smi),True) for smi in x_valid]\n",
        "x_test = [Chem.MolToSmiles(Chem.MolFromSmiles(smi),True) for smi in x_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmAYCS0q_ZZx"
      },
      "outputs": [],
      "source": [
        "train_dataset = pd.concat([pd.DataFrame(x_train), pd.DataFrame(y_train)], axis = 1)\n",
        "train_dataset.columns = ['smiles','p_np']\n",
        "x_train = list(train_dataset.smiles.tolist())\n",
        "y_train = list(train_dataset.p_np.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dpl1kNJ_Zce"
      },
      "outputs": [],
      "source": [
        "# create class weights\n",
        "from sklearn.utils import class_weight\n",
        "y_unique = np.unique(np.array(y_train))\n",
        "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',classes = y_unique,\n",
        "                       y = np.array(y_train)) \n",
        "class_weights_dict45 = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqF_TkEp9aQ-"
      },
      "outputs": [],
      "source": [
        "# get the tokens \n",
        "x_train_list = []\n",
        "for i in x_train:\n",
        "  x_train_list.append(tokenizer(i)['input_ids'])\n",
        "\n",
        "x_valid_list = []\n",
        "for i in x_valid:\n",
        "  x_valid_list.append(tokenizer(i)['input_ids'])  \n",
        "\n",
        "x_test_list = []\n",
        "for i in x_test:\n",
        "  x_test_list.append(tokenizer(i)['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZERQ62tl9ft_"
      },
      "outputs": [],
      "source": [
        "# zero padding \n",
        "x_train_list_128 = []\n",
        "for i in x_train_list:\n",
        "  x_train_list_128.append(np.pad(i, (0, 128 - len(i)), 'constant', constant_values = 0))\n",
        "\n",
        "x_valid_list_128 = []\n",
        "for i in x_valid_list:\n",
        "  x_valid_list_128.append(np.pad(i, (0, 128 - len(i)), 'constant', constant_values = 0))\n",
        "\n",
        "x_test_list_128 = []\n",
        "for i in x_test_list:\n",
        "  x_test_list_128.append(np.pad(i, (0, 128 - len(i)), 'constant', constant_values = 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U_LuolqbbjA"
      },
      "outputs": [],
      "source": [
        "# get the train, valid, test sets\n",
        "x_train = np.array(x_train_list_128)  \n",
        "x_valid = np.array(x_valid_list_128)  \n",
        "x_test = np.array(x_test_list_128) \n",
        "\n",
        "y_train = np.array(y_train).astype(int)\n",
        "y_valid = np.array(y_valid).astype(int)\n",
        "y_test = np.array(y_test).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YdqrrPAca5V"
      },
      "outputs": [],
      "source": [
        "# set the architecture of model   \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, LSTM, Conv1D, Bidirectional, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(3133, output_dim = 128))\n",
        "model.add(Bidirectional(LSTM(128, dropout = 0.5)))\n",
        "model.add(Dropout(0.96))\n",
        "model.add(Dense(len(set(y_train)), activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "       metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKZZuWSHMne6"
      },
      "outputs": [],
      "source": [
        "# set the checkpoint \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath_lstm = './content/LSTM_top_20_MOA_weights.hdf5'\n",
        "checkpoint_lstm = ModelCheckpoint(filepath_lstm, monitor='val_accuracy', verbose=0, save_best_only=True,\n",
        "                  mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I48EP50aftnF"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "from tensorflow.keras.callbacks import EarlyStopping  \n",
        "earlyStopping = EarlyStopping(monitor = 'val_loss', patience = 25, verbose = 0, mode = 'min')\n",
        "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.6, \n",
        "          patience = 7, verbose = 1, min_delta = 1e-119, mode = 'min')\n",
        "history = model.fit(x_train, y_train, validation_data = (x_valid, y_valid), \n",
        "          class_weight = class_weights_dict45, shuffle = True, verbose = 2, epochs = 500,\n",
        "          batch_size = 128, callbacks = [earlyStopping, checkpoint_lstm, reduce_lr_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSfVS5_POy8O"
      },
      "outputs": [],
      "source": [
        "# load the best model\n",
        "from keras.models import load_model\n",
        "best_model = load_model(filepath_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI2IWPm04MXm"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model \n",
        "from sklearn.metrics import classification_report\n",
        "assert list(y_test)[0:5] == [14, 12, 6, 13, 14]\n",
        "print(classification_report(y_test, np.array(best_model.predict(x_test).argmax(-1)),))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09gspmb7WkUT"
      },
      "outputs": [],
      "source": [
        "# Training curves\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc = 'upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc = 'upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgv_gh6sa38J"
      },
      "outputs": [],
      "source": [
        "# References \n",
        "# https://colab.research.google.com/drive/1tsiTpC4i26QNdRzBHFfXIOFVToE54-9b?usp=sharing#scrollTo=UHzrWuFpCtzs\n",
        "# https://github.com/XinhaoLi74/SmilesPE"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "chem-moa",
      "language": "python",
      "name": "chem-moa"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}