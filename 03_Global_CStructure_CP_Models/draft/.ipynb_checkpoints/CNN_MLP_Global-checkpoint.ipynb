{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOCTxJS5_fM9"
   },
   "outputs": [],
   "source": [
    "# 10 selected MoAs \n",
    "moas_to_use = ['Aurora kinase inhibitor', 'tubulin polymerization inhibitor', 'JAK inhibitor', 'protein synthesis inhibitor', 'HDAC inhibitor', \n",
    "        'topoisomerase inhibitor', 'PARP inhibitor', 'ATPase inhibitor', 'retinoid receptor agonist', 'HSP inhibitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lon5FqMB3JXq"
   },
   "outputs": [],
   "source": [
    "# read the data \n",
    "import pandas as pd\n",
    "all_data = pd.read_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEL4xND9DG9y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhksy76-KxaI"
   },
   "outputs": [],
   "source": [
    "# get the compound-MoA pair \n",
    "compound_moa = all_data[['compound','classes']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp16IyZKKxdf"
   },
   "outputs": [],
   "source": [
    "# split out the test compounds \n",
    "from sklearn.model_selection import train_test_split\n",
    "compound_train_valid, compound_test, moa_train_valid, moa_test = train_test_split(\n",
    "  compound_moa.compound, compound_moa.classes, test_size = 0.10, stratify = compound_moa.classes, \n",
    "  shuffle = True, random_state = 1)\n",
    "assert list(moa_test[0:5]) == [7, 5, 1, 9, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgZQa4pTO7nE"
   },
   "outputs": [],
   "source": [
    "# get the compounds for training and validation      \n",
    "compound_train, compound_valid, moa_train, moa_valid = train_test_split(\n",
    "  compound_train_valid, moa_train_valid, test_size = 10/90, stratify = moa_train_valid,\n",
    "  shuffle = True, random_state = 62757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73_IwUtfO7sm"
   },
   "outputs": [],
   "source": [
    "# get the train, valid and test set \n",
    "train = all_data[all_data['compound'].isin(compound_train)].reset_index(drop = True)  \n",
    "valid = all_data[all_data['compound'].isin(compound_valid)].reset_index(drop = True)  \n",
    "test = all_data[all_data['compound'].isin(compound_test)].reset_index(drop = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpaHDV_NyUTz"
   },
   "outputs": [],
   "source": [
    "# get the dictionary for compound_id-SMILES pair \n",
    "import pickle\n",
    "compound_smiles_dictionary = pickle.load(open(\"/content/dictionary2.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDGCruBJA1J2"
   },
   "outputs": [],
   "source": [
    "# on the fly data augmentation \n",
    "import albumentations as A\n",
    "train_transforms = A.Compose([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.2),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.4),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.5),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.6),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.8),\n",
    "    A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),])\n",
    "valid_transforms = A.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDZ2gf4WCxcW"
   },
   "outputs": [],
   "source": [
    "# get all images (12582)\n",
    "import numpy as np\n",
    "#all_images = np.load(open(\"/content/drive/MyDrive/github/all_images.npy\", \"rb\"))\n",
    "all_images = np.load(open(, \"rb\")) # Path has to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOHasY7qCrm4"
   },
   "outputs": [],
   "source": [
    "# data generator for training \n",
    "def get_train_image(end):\n",
    "  start = 0\n",
    "  while start < end:      \n",
    "    idx = start       \n",
    "    row = train.iloc[idx]\n",
    "\n",
    "    assert row['digit']     in train.digit.tolist()\n",
    "    assert row['compound']   in train.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in valid.digit.tolist()\n",
    "    assert row['compound']  not in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in test.digit.tolist()\n",
    "    assert row['compound']  not in test.compound.tolist()\n",
    "\n",
    "    image = all_images[all_data.digit.tolist().index(row['digit'])]  \n",
    "    image = train_transforms(image = image)['image']               \n",
    "    target = int(row['classes'])   \n",
    "    \n",
    "    yield image, target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbPwaU-dCro5"
   },
   "outputs": [],
   "source": [
    "# data generator for validation   \n",
    "def get_valid_image(end):\n",
    "  start = 0\n",
    "  while start<end:\n",
    "    idx = start       \n",
    "    row = valid.iloc[idx]\n",
    "\n",
    "    assert row['digit']   not in train.digit.tolist()\n",
    "    assert row['compound']  not in train.compound.tolist()\n",
    "\n",
    "    assert row['digit']     in valid.digit.tolist()\n",
    "    assert row['compound']   in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in test.digit.tolist()\n",
    "    assert row['compound']  not in test.compound.tolist()\n",
    "       \n",
    "    image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "    image = valid_transforms(image = image)['image']\n",
    "    target = int(row['classes'])   \n",
    "    \n",
    "    yield image, target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mz8y7LJCrrG"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "channels = 5      \n",
    "image_size = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6agIQbDGkw3"
   },
   "outputs": [],
   "source": [
    "# turn to tensorflow datasets \n",
    "import tensorflow as tf\n",
    "train_data = tf.data.Dataset.from_generator(get_train_image,\n",
    "            (tf.float32, tf.int32),\n",
    "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
    "            args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)\n",
    "\n",
    "valid_data = tf.data.Dataset.from_generator(get_valid_image,\n",
    "            (tf.float32, tf.int32),\n",
    "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
    "            args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPiZ1G_oG2Rl"
   },
   "outputs": [],
   "source": [
    "# we choose efficientnet b1 as the base model    \n",
    "base_model = tf.keras.applications.EfficientNetB1(input_shape = (image_size, image_size, channels),\n",
    "                                  include_top = False, weights = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZNg2epx1h_T"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-FX8pid1h_U"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbRdHCZx1h_U"
   },
   "outputs": [],
   "source": [
    "# complete the architecture of efficientnet b1\n",
    "drop = 0.30\n",
    "num_classes = len(set(train['classes'].tolist())) \n",
    "x = base_model.output\n",
    "x = Dropout(drop)(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = Dropout(drop)(x)\n",
    "preds = layers.Dense(num_classes, activation = 'softmax',                     \n",
    "    kernel_regularizer = regularizers.L1L2(l1 = 1e-4, l2 = 1e-3),\n",
    "    bias_regularizer = regularizers.L2(1e-3),\n",
    "    activity_regularizer = regularizers.L2(1e-4))(x)\n",
    "cnn_model = models.Model(inputs = base_model.input, outputs = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd4PkWWJ1h_V"
   },
   "outputs": [],
   "source": [
    "# set the optimizer of efficientnet b1    \n",
    "cnn_optimizer = tfa.optimizers.AdamW(weight_decay = 1e-6, learning_rate = 0.001, beta_1 = 0.9,\n",
    "    beta_2 = 0.999, epsilon = 1e-07,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLZSv4icG6Qv"
   },
   "outputs": [],
   "source": [
    "# compile the model   \n",
    "cnn_model.compile(optimizer = cnn_optimizer,\n",
    "         loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "         metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2s1BvcYG--W"
   },
   "outputs": [],
   "source": [
    "# set the class weights \n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', \n",
    "         classes = np.unique(train.classes), y = train.classes)   \n",
    "weight_dictionary = dict(zip(np.unique(train.classes), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buV4eg24HERe"
   },
   "outputs": [],
   "source": [
    "# set the check point   \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath_cnn = './content/shuffle_5_' + str(base_model.name) + '_weights.hdf5'\n",
    "checkpoint_cnn = ModelCheckpoint(filepath_cnn, monitor = 'val_accuracy', verbose = 0, \n",
    "                  save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecAtvKNaHVec"
   },
   "outputs": [],
   "source": [
    "# train the efficientnet b1\n",
    "from tensorflow.keras.callbacks import EarlyStopping  \n",
    "reduce_lr_loss_cnn = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
    "              factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
    "history_cnn = cnn_model.fit(train_data, validation_data = valid_data, class_weight = weight_dictionary,\n",
    "               verbose = 2, epochs = 100, callbacks=[reduce_lr_loss_cnn, checkpoint_cnn,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATgS1vCJp9ko"
   },
   "outputs": [],
   "source": [
    "# get the best model \n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "#best_cnn_model = load_model('/content/drive/MyDrive/github/shuffle_5_efficientnetb1_weights.hdf5')\n",
    "best_cnn_model = load_model(\"\") # Path has to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPopQovGLhYp"
   },
   "outputs": [],
   "source": [
    "# evaluate the model   \n",
    "predicted_test = []\n",
    "for i in range(test.shape[0]):    \n",
    "  row = test.iloc[i]\n",
    "\n",
    "  assert row['digit']  not  in train.digit.tolist()\n",
    "  assert row['compound'] not  in train.compound.tolist()\n",
    "  \n",
    "  assert row['digit']   not  in valid.digit.tolist()\n",
    "  assert row['compound']  not  in valid.compound.tolist()\n",
    "\n",
    "  assert row['digit']     in test.digit.tolist()\n",
    "  assert row['compound']    in test.compound.tolist()  \n",
    "\n",
    "  im = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "  im = valid_transforms(image = im)['image']\n",
    "  im = np.expand_dims(im, 0)\n",
    "  value = best_cnn_model.predict(im).argmax()\n",
    "  \n",
    "  predicted_test.append(value)   \n",
    "  \n",
    "print(classification_report(test.classes.tolist(), predicted_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2KGpukC8j8F"
   },
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BV_BLxHFrRDC"
   },
   "outputs": [],
   "source": [
    "# A function changing SMILES to Morgan fingerprints \n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import DataStructs, AllChem\n",
    "def smiles_to_array(smiles):\n",
    "  molecules = Chem.MolFromSmiles(smiles) \n",
    "  fingerprints = AllChem.GetMorganFingerprintAsBitVect(molecules, 2)\n",
    "  x_array = []\n",
    "  arrays = np.zeros(0,)\n",
    "  DataStructs.ConvertToNumpyArray(fingerprints, arrays)\n",
    "  x_array.append(arrays)\n",
    "  x_array = np.asarray(x_array)\n",
    "  x_array = ((np.squeeze(x_array)).astype(int)) \n",
    "  return x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJhVdYD9rRFZ"
   },
   "outputs": [],
   "source": [
    "# generator for training data \n",
    "def get_train_smiles(end):\n",
    "  start = 0\n",
    "  while start < end:\n",
    "    idx = start       \n",
    "    row = train.iloc[idx]\n",
    "\n",
    "    assert row['digit']    in train.digit.tolist()\n",
    "    assert row['compound']   in train.compound.tolist()\n",
    "  \n",
    "    assert row['digit']    not in valid.digit.tolist()\n",
    "    assert row['compound']  not  in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']   not  in test.digit.tolist()\n",
    "    assert row['compound']  not  in test.compound.tolist()\n",
    "\n",
    "    smiles = compound_smiles_dictionary[row['compound']]     \n",
    "    smiles_array = smiles_to_array(smiles)                      \n",
    "    target_mlp = int(row['classes'])   \n",
    "    \n",
    "    yield smiles_array, target_mlp\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpo4_jw-rRIP"
   },
   "outputs": [],
   "source": [
    "# generator for validation data \n",
    "def get_valid_smiles(end):\n",
    "  start = 0\n",
    "  while start<end:\n",
    "    idx = start       \n",
    "    row = valid.iloc[idx]\n",
    "\n",
    "    assert row['digit']  not  in train.digit.tolist()\n",
    "    assert row['compound'] not  in train.compound.tolist()\n",
    "  \n",
    "    assert row['digit']     in valid.digit.tolist()\n",
    "    assert row['compound']    in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']   not  in test.digit.tolist()\n",
    "    assert row['compound']  not  in test.compound.tolist()\n",
    "\n",
    "    smiles = compound_smiles_dictionary[row['compound']]     \n",
    "    smiles_array = smiles_to_array(smiles)                  \n",
    "    target_mlp = int(row['classes'])   \n",
    "    \n",
    "    yield smiles_array, target_mlp\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMImnijyrRRi"
   },
   "outputs": [],
   "source": [
    "# complete the architecture of MLP and compile MLP \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "units = 64  \n",
    "drop = 0.89  \n",
    "\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(units, input_dim = 2048, activation = 'relu'))\n",
    "model_mlp.add(Dropout(drop))\n",
    "model_mlp.add(Dense(10, activation = 'softmax'))\n",
    "model_mlp.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4),\n",
    "         loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "         metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQx7kbHXrRUg"
   },
   "outputs": [],
   "source": [
    "# turn to TensorFlow dataset \n",
    "train_smiles_data = tf.data.Dataset.from_generator(get_train_smiles,\n",
    "            (tf.float32, tf.int32),\n",
    "            (tf.TensorShape(2048), tf.TensorShape([])),\n",
    "            args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNVfUd1ywrhY"
   },
   "outputs": [],
   "source": [
    "# turn to TensorFlow dataset  \n",
    "valid_smiles_data = tf.data.Dataset.from_generator(get_valid_smiles,\n",
    "            (tf.float32, tf.int32),\n",
    "            (tf.TensorShape(2048), tf.TensorShape([])),\n",
    "            args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VI6KDDQ_xr1Q"
   },
   "outputs": [],
   "source": [
    "# set the checkpoint\n",
    "filepath_mlp = './content/shuffle_5_mlp_weights.hdf5'\n",
    "checkpoint_mlp = ModelCheckpoint(filepath_mlp, monitor = 'val_accuracy', verbose = 0, \n",
    "                  save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5j9lXwfw-yV"
   },
   "outputs": [],
   "source": [
    "# train the model  \n",
    "from tensorflow.keras.callbacks import EarlyStopping  \n",
    "earlyStopping = EarlyStopping(monitor = 'val_loss', patience = 20, verbose = 2, mode = 'min')\n",
    "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
    "history_mlp = model_mlp.fit(train_smiles_data, validation_data = valid_smiles_data,\n",
    "               class_weight = weight_dictionary, verbose = 2, epochs = 1800,      \n",
    "               callbacks = [earlyStopping, reduce_lr_loss, checkpoint_mlp,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FG4jMIM2o5_v"
   },
   "outputs": [],
   "source": [
    "# get the best MLP and evaluate it  \n",
    "#best_model_mlp = load_model('/content/drive/MyDrive/github/shuffle_5_mlp_weights.hdf5') \n",
    "best_model_mlp = load_model('') # Path has to be set\n",
    "predicted_test = []\n",
    "for i in range(test.shape[0]): \n",
    "  row = test.iloc[i]\n",
    "\n",
    "  assert row['digit']  not  in train.digit.tolist()\n",
    "  assert row['compound'] not  in train.compound.tolist()\n",
    "  \n",
    "  assert row['digit']   not  in valid.digit.tolist()\n",
    "  assert row['compound']  not   in valid.compound.tolist()\n",
    "\n",
    "  assert row['digit']     in test.digit.tolist()\n",
    "  assert row['compound']    in test.compound.tolist()\n",
    "  \n",
    "  smiles = compound_smiles_dictionary[row['compound']]     \n",
    "  smiles_array = smiles_to_array(smiles)  \n",
    "  smiles_array = np.expand_dims(smiles_array, 0)\n",
    "  value = int(best_model_mlp.predict(smiles_array)[0].argmax())\n",
    "\n",
    "  predicted_test.append(value) \n",
    "\n",
    "print(classification_report(test.classes.tolist(), predicted_test))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlP8CMo59cL2"
   },
   "source": [
    "Integrate MLP and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgaVu4nv2LaX"
   },
   "outputs": [],
   "source": [
    "# training data generator for the global model \n",
    "def get_train_image_smiles(end):\n",
    "  start = 0\n",
    "  while start < end:\n",
    "    idx = start       \n",
    "    row = train.iloc[idx]\n",
    "\n",
    "    assert row['digit']    in train.digit.tolist()\n",
    "    assert row['compound']   in train.compound.tolist()\n",
    "  \n",
    "    assert row['digit']   not  in valid.digit.tolist()\n",
    "    assert row['compound']  not  in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']   not  in test.digit.tolist()\n",
    "    assert row['compound']  not  in test.compound.tolist()\n",
    "\n",
    "    image  = all_images[all_data.digit.tolist().index(row['digit'])]  \n",
    "    image = train_transforms(image = image)['image'] \n",
    "\n",
    "    smiles = compound_smiles_dictionary[row['compound']]     \n",
    "    smiles_array = smiles_to_array(smiles)                \n",
    "    target = int(row['classes'])   \n",
    "\n",
    "    yield (image, smiles_array), target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZVdj5kZ3k-b"
   },
   "outputs": [],
   "source": [
    "# validation data generator for the global model \n",
    "def get_valid_image_smiles(end):\n",
    "  start = 0\n",
    "  while start < end:\n",
    "    idx = start       \n",
    "    row = valid.iloc[idx]\n",
    "       \n",
    "    assert row['digit']  not  in train.digit.tolist()\n",
    "    assert row['compound'] not  in train.compound.tolist()\n",
    "  \n",
    "    assert row['digit']     in valid.digit.tolist()\n",
    "    assert row['compound']    in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']   not  in test.digit.tolist()\n",
    "    assert row['compound']  not  in test.compound.tolist()       \n",
    "    \n",
    "    image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "    image = valid_transforms(image = image)['image']\n",
    "    smiles = compound_smiles_dictionary[row['compound']]     \n",
    "    smiles_array = smiles_to_array(smiles)      \n",
    "    target = int(row['classes'])   \n",
    "\n",
    "    yield (image, smiles_array), target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4OLEZT14QHc"
   },
   "outputs": [],
   "source": [
    "# turn to TensorFlow dataset \n",
    "train_image_smiles = tf.data.Dataset.from_generator(get_train_image_smiles,\n",
    "           ((tf.float32, tf.float32), tf.int32),\n",
    "           ((tf.TensorShape([image_size, image_size, channels]), 2048), tf.TensorShape([])),\n",
    "           args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Onk6f8Ca5IkW"
   },
   "outputs": [],
   "source": [
    "# turn to TensorFlow dataset \n",
    "valid_image_smiles = tf.data.Dataset.from_generator(get_valid_image_smiles,\n",
    "           ((tf.float32, tf.float32), tf.int32),\n",
    "           ((tf.TensorShape([image_size, image_size, channels]), 2048), tf.TensorShape([])),\n",
    "           args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znKoL9qx5IpN"
   },
   "outputs": [],
   "source": [
    "# combine MLP and efficientNet to get the global model    \n",
    "from keras import Model\n",
    "model1 = Model(inputs = best_cnn_model.input, \n",
    "        outputs = best_cnn_model.get_layer('global_average_pooling2d').output)\n",
    "\n",
    "model2 = Model(inputs = best_model_mlp.input, outputs = best_model_mlp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oUY_lzH54lN"
   },
   "outputs": [],
   "source": [
    "x1 = model1.output    \n",
    "x1 = tf.keras.layers.Dense(64, activation = 'relu')(x1)    \n",
    "x2 = model2.output \n",
    "x3 = tf.concat([x1, x2], axis = -1)\n",
    "x3 = tf.keras.layers.Dense(128, activation = 'relu')(x3)\n",
    "output1 = tf.keras.layers.Dense(10, activation = 'softmax')(x3)\n",
    "global_model = tf.keras.models.Model(inputs = [model1.input, model2.input], outputs = output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2wPRruh7n8R"
   },
   "outputs": [],
   "source": [
    "# freeze MLP and efficientnet \n",
    "model1.trainable = False\n",
    "model2.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIYaShcN8nqG"
   },
   "outputs": [],
   "source": [
    "# compile the global model        \n",
    "global_model.compile(optimizer = cnn_optimizer, \n",
    "           loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "           metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfZJ8BNu9FJK"
   },
   "outputs": [],
   "source": [
    "# train the global model with MLP and efficientnet freezed     \n",
    "history_global = global_model.fit(train_image_smiles, validation_data = valid_image_smiles,     \n",
    "                  class_weight = weight_dictionary, verbose = 2, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppE-GTdjA8Tu"
   },
   "outputs": [],
   "source": [
    "# Unfreeze MLP and efficientNet   \n",
    "model1.trainable = True\n",
    "model2.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qby3nbTrBWc5"
   },
   "outputs": [],
   "source": [
    "# set the optimizer\n",
    "global_model_optimizer = tfa.optimizers.AdamW(weight_decay = 1e-6, learning_rate = 1e-3/2,\n",
    "                         beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVSpatc9BgTD"
   },
   "outputs": [],
   "source": [
    "# compile the global model            \n",
    "global_model.compile(optimizer = global_model_optimizer, \n",
    "           loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "           metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQpjxQ_lCLRH"
   },
   "outputs": [],
   "source": [
    "# set the checkpoint   \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath = './content/shuffle_5_global_weights.hdf5'\n",
    "global_model_checkpoint = ModelCheckpoint(filepath, monitor = 'val_accuracy', verbose = 0, \n",
    "                      save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyGMxloIChNl"
   },
   "outputs": [],
   "source": [
    "# train the global model \n",
    "reduce_lr_loss_global = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
    "             factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
    "history_global = global_model.fit(train_image_smiles, validation_data = valid_image_smiles,     \n",
    "             class_weight = weight_dictionary, verbose = 2, epochs = 100-10,\n",
    "             callbacks = [reduce_lr_loss_global, global_model_checkpoint,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsGlHozb_-RB"
   },
   "outputs": [],
   "source": [
    "# load the best global model and evaluate it \n",
    "#best_global_model = load_model('/content/drive/MyDrive/github/shuffle_5_global_weights.hdf5')\n",
    "best_global_model = load_model('') # Path has to be set\n",
    "predicted_test = []\n",
    "for i in range(test.shape[0]):\n",
    "  row = test.iloc[i]\n",
    "\n",
    "  assert row['digit']  not  in train.digit.tolist()\n",
    "  assert row['compound'] not  in train.compound.tolist()\n",
    "  \n",
    "  assert row['digit']    not in valid.digit.tolist()\n",
    "  assert row['compound']  not  in valid.compound.tolist()\n",
    "\n",
    "  assert row['digit']     in test.digit.tolist()\n",
    "  assert row['compound']    in test.compound.tolist()\n",
    "\n",
    "  image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "  image = valid_transforms(image = image)['image']\n",
    "  smiles = compound_smiles_dictionary[row['compound']]     \n",
    "  smiles_array = smiles_to_array(smiles) \n",
    "\n",
    "  image = np.expand_dims(image, 0)\n",
    "  smiles_array = np.expand_dims(smiles_array, 0)       \n",
    "\n",
    "  value = best_global_model.predict((image, smiles_array)).argmax()   \n",
    "  predicted_test.append(value)   \n",
    "\n",
    "print(classification_report(test.classes.tolist(), predicted_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kw9fcxklW2Y"
   },
   "outputs": [],
   "source": [
    "# references \n",
    "# https://www.tensorflow.org/guide/data\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "# https://www.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
