{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOCTxJS5_fM9"
   },
   "outputs": [],
   "source": [
    "# 10 selected MoAs \n",
    "moas_to_use = ['Aurora kinase inhibitor', 'tubulin polymerization inhibitor', 'JAK inhibitor', 'protein synthesis inhibitor', 'HDAC inhibitor', \n",
    "        'topoisomerase inhibitor', 'PARP inhibitor', 'ATPase inhibitor', 'retinoid receptor agonist', 'HSP inhibitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lon5FqMB3JXq"
   },
   "outputs": [],
   "source": [
    "# read the data \n",
    "import pandas as pd\n",
    "all_data = pd.read_csv('../Cell_morphology_based_model_and_global_model/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEL4xND9DG9y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhksy76-KxaI"
   },
   "outputs": [],
   "source": [
    "# get the compound-MoA pair \n",
    "compound_moa = all_data[['compound','classes']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp16IyZKKxdf"
   },
   "outputs": [],
   "source": [
    "# split out the test compounds \n",
    "from sklearn.model_selection import train_test_split\n",
    "compound_train_valid, compound_test, moa_train_valid, moa_test = train_test_split(\n",
    "  compound_moa.compound, compound_moa.classes, test_size = 0.10, stratify = compound_moa.classes, \n",
    "  shuffle = True, random_state = 1)\n",
    "assert list(moa_test[0:5]) == [7, 5, 1, 9, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgZQa4pTO7nE"
   },
   "outputs": [],
   "source": [
    "# get the compounds for training and validation      \n",
    "compound_train, compound_valid, moa_train, moa_valid = train_test_split(\n",
    "  compound_train_valid, moa_train_valid, test_size = 10/90, stratify = moa_train_valid,\n",
    "  shuffle = True, random_state = 62757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73_IwUtfO7sm"
   },
   "outputs": [],
   "source": [
    "# get the train, valid and test set \n",
    "train = all_data[all_data['compound'].isin(compound_train)].reset_index(drop = True)  \n",
    "valid = all_data[all_data['compound'].isin(compound_valid)].reset_index(drop = True)  \n",
    "test = all_data[all_data['compound'].isin(compound_test)].reset_index(drop = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpaHDV_NyUTz"
   },
   "outputs": [],
   "source": [
    "# get the dictionary for compound_id-SMILES pair \n",
    "import pickle\n",
    "compound_smiles_dictionary = pickle.load(open(\"../Cell_morphology_based_model_and_global_model/dictionary2.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDGCruBJA1J2"
   },
   "outputs": [],
   "source": [
    "# on the fly data augmentation \n",
    "import albumentations as A\n",
    "train_transforms = A.Compose([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.2),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.4),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.5),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.6),\n",
    "    A.OneOf([A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),],p = 0.8),\n",
    "    A.Flip(),A.ShiftScaleRotate(scale_limit=0.2),A.RandomRotate90(),])\n",
    "valid_transforms = A.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDZ2gf4WCxcW"
   },
   "outputs": [],
   "source": [
    "# get all images (12582)\n",
    "import numpy as np\n",
    "all_images = np.load(open(, \"rb\")) # The path has to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOHasY7qCrm4"
   },
   "outputs": [],
   "source": [
    "# data generator for training \n",
    "def get_train_image(end):\n",
    "  start = 0\n",
    "  while start < end:      \n",
    "    idx = start       \n",
    "    row = train.iloc[idx]\n",
    "\n",
    "    assert row['digit']     in train.digit.tolist()\n",
    "    assert row['compound']   in train.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in valid.digit.tolist()\n",
    "    assert row['compound']  not in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in test.digit.tolist()\n",
    "    assert row['compound']  not in test.compound.tolist()\n",
    "\n",
    "    image = all_images[all_data.digit.tolist().index(row['digit'])]  \n",
    "    image = train_transforms(image = image)['image']               \n",
    "    target = int(row['classes'])   \n",
    "    \n",
    "    yield image, target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbPwaU-dCro5"
   },
   "outputs": [],
   "source": [
    "# data generator for validation   \n",
    "def get_valid_image(end):\n",
    "  start = 0\n",
    "  while start<end:\n",
    "    idx = start       \n",
    "    row = valid.iloc[idx]\n",
    "\n",
    "    assert row['digit']   not in train.digit.tolist()\n",
    "    assert row['compound']  not in train.compound.tolist()\n",
    "\n",
    "    assert row['digit']     in valid.digit.tolist()\n",
    "    assert row['compound']   in valid.compound.tolist()\n",
    "\n",
    "    assert row['digit']    not in test.digit.tolist()\n",
    "    assert row['compound']  not in test.compound.tolist()\n",
    "       \n",
    "    image = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "    image = valid_transforms(image = image)['image']\n",
    "    target = int(row['classes'])   \n",
    "    \n",
    "    yield image, target\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mz8y7LJCrrG"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "channels = 5      \n",
    "image_size = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6agIQbDGkw3"
   },
   "outputs": [],
   "source": [
    "# turn to tensorflow datasets \n",
    "import tensorflow as tf\n",
    "train_data = tf.data.Dataset.from_generator(get_train_image,\n",
    "            (tf.float32, tf.int32),\n",
    "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
    "            args = [train.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)\n",
    "\n",
    "valid_data = tf.data.Dataset.from_generator(get_valid_image,\n",
    "            (tf.float32, tf.int32),\n",
    "            ((tf.TensorShape([image_size, image_size, channels])), tf.TensorShape([])),\n",
    "            args = [valid.shape[0]]).batch(batch_size, num_parallel_calls = 64).prefetch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPiZ1G_oG2Rl"
   },
   "outputs": [],
   "source": [
    "# we choose efficientnet b1 as the base model    \n",
    "base_model = tf.keras.applications.EfficientNetB1(input_shape = (image_size, image_size, channels),\n",
    "                                  include_top = False, weights = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-FX8pid1h_U"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbRdHCZx1h_U"
   },
   "outputs": [],
   "source": [
    "# complete the architecture of efficientnet b1\n",
    "drop = 0.30\n",
    "num_classes = len(set(train['classes'].tolist())) \n",
    "x = base_model.output\n",
    "x = Dropout(drop)(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = Dropout(drop)(x)\n",
    "preds = layers.Dense(num_classes, activation = 'softmax',                     \n",
    "    kernel_regularizer = regularizers.L1L2(l1 = 1e-4, l2 = 1e-3),\n",
    "    bias_regularizer = regularizers.L2(1e-3),\n",
    "    activity_regularizer = regularizers.L2(1e-4))(x)\n",
    "cnn_model = models.Model(inputs = base_model.input, outputs = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd4PkWWJ1h_V"
   },
   "outputs": [],
   "source": [
    "# set the optimizer of efficientnet b1    \n",
    "cnn_optimizer = tfa.optimizers.AdamW(weight_decay = 1e-6, learning_rate = 0.001, beta_1 = 0.9,\n",
    "    beta_2 = 0.999, epsilon = 1e-07,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLZSv4icG6Qv"
   },
   "outputs": [],
   "source": [
    "# compile the model   \n",
    "cnn_model.compile(optimizer = cnn_optimizer,\n",
    "         loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "         metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2s1BvcYG--W"
   },
   "outputs": [],
   "source": [
    "# set the class weights \n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', \n",
    "         classes = np.unique(train.classes), y = train.classes)   \n",
    "weight_dictionary = dict(zip(np.unique(train.classes), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buV4eg24HERe"
   },
   "outputs": [],
   "source": [
    "# set the check point   \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath_cnn = './content/shuffle_5_' + str(base_model.name) + '_weights.hdf5'\n",
    "checkpoint_cnn = ModelCheckpoint(filepath_cnn, monitor = 'val_accuracy', verbose = 0, \n",
    "                  save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecAtvKNaHVec"
   },
   "outputs": [],
   "source": [
    "# train the efficientnet b1\n",
    "from tensorflow.keras.callbacks import EarlyStopping  \n",
    "reduce_lr_loss_cnn = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n",
    "              factor = 0.5, patience = 9, verbose = 2, min_lr = 1e-7, mode = 'min')\n",
    "history_cnn = cnn_model.fit(train_data, validation_data = valid_data, class_weight = weight_dictionary,\n",
    "               verbose = 2, epochs = 100, callbacks=[reduce_lr_loss_cnn, checkpoint_cnn,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATgS1vCJp9ko"
   },
   "outputs": [],
   "source": [
    "# get the best model \n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "best_cnn_model = load_model() # Path has to be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPopQovGLhYp"
   },
   "outputs": [],
   "source": [
    "# evaluate the model   \n",
    "predicted_test = []\n",
    "for i in range(test.shape[0]):    \n",
    "  row = test.iloc[i]\n",
    "\n",
    "  assert row['digit']  not  in train.digit.tolist()\n",
    "  assert row['compound'] not  in train.compound.tolist()\n",
    "  \n",
    "  assert row['digit']   not  in valid.digit.tolist()\n",
    "  assert row['compound']  not  in valid.compound.tolist()\n",
    "\n",
    "  assert row['digit']     in test.digit.tolist()\n",
    "  assert row['compound']    in test.compound.tolist()  \n",
    "\n",
    "  im = all_images[all_data.digit.tolist().index(row['digit'])] \n",
    "  im = valid_transforms(image = im)['image']\n",
    "  im = np.expand_dims(im, 0)\n",
    "  value = best_cnn_model.predict(im).argmax()\n",
    "  \n",
    "  predicted_test.append(value)   \n",
    "  \n",
    "print(classification_report(test.classes.tolist(), predicted_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kw9fcxklW2Y"
   },
   "outputs": [],
   "source": [
    "# references \n",
    "# https://www.tensorflow.org/guide/data\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "# https://www.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "chem-moa",
   "language": "python",
   "name": "chem-moa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
