{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy as dp\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "# TabNet\n",
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n",
    "\n",
    "# Tabnet \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# feature transformation - fit\n",
    "def norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n",
    "    ss_1_dic = {'zsco':StandardScaler(),\n",
    "                'mima':MinMaxScaler(),\n",
    "                'maxb':MaxAbsScaler(), \n",
    "                'robu':RobustScaler(),\n",
    "                'norm':Normalizer(), \n",
    "                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n",
    "                'powe':PowerTransformer()}\n",
    "    ss_1 = ss_1_dic[sc_name]\n",
    "    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    if saveM == False:\n",
    "        return(df_2)\n",
    "    else:\n",
    "        return(df_2,ss_1)\n",
    "\n",
    "# feature transformation - trans\n",
    "def norm_tra(df_1,ss_x):\n",
    "    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n",
    "    return(df_2)\n",
    "\n",
    "# frequency \n",
    "def f_table(list1):\n",
    "    table_dic = {}\n",
    "    for i in list1:\n",
    "        if i not in table_dic.keys():\n",
    "            table_dic[i] = 1\n",
    "        else:\n",
    "            table_dic[i] += 1\n",
    "    return(table_dic)\n",
    "\n",
    "# seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "# input data dir\n",
    "input_dir = '../input/lish-moa/'\n",
    "# upload model dataset from training kernel outputs\n",
    "mod_path1 = '../input/ble2-dro1-3-ns1-4-tr2/'                 # 1D-CNN\n",
    "mod_path2 = '../input/fork-of-ble-w1-6-2-kd-tab1-wgt1-pa1-4/' # TabNet\n",
    "mod_path3 = '../input/ble2-nntrans-tr/'                       # DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod1: 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "sc_dic = {}\n",
    "feat_dic = {}\n",
    "train_features = pd.read_csv(input_dir+'train_features.csv')\n",
    "train_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv(input_dir+'test_features.csv')\n",
    "sample_submission = pd.read_csv(input_dir+'sample_submission.csv')\n",
    "train_drug = pd.read_csv(input_dir+'train_drug.csv')\n",
    "\n",
    "target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "# non-score targets highly correlated with scored targets will be used in pretrain\n",
    "nonctr_id = train_features.loc[train_features['cp_type']!='ctl_vehicle','sig_id'].tolist()\n",
    "tmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\n",
    "mat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n",
    "                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\n",
    "mat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\n",
    "mat_cor2.index = target_nonsc_cols\n",
    "mat_cor2.columns = target_cols\n",
    "mat_cor2 = mat_cor2.dropna()\n",
    "mat_cor2_max = mat_cor2.abs().max(axis = 1)\n",
    "\n",
    "q_n_cut = 0.9\n",
    "target_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\n",
    "print(len(target_nonsc_cols2))\n",
    "\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "feat_dic['gene'] = GENES\n",
    "feat_dic['cell'] = CELLS\n",
    "\n",
    "# sample normalization \n",
    "q2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\n",
    "q2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n",
    "\n",
    "q2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "q2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "# remove ctr \n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n",
    "\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[['sig_id']+target_cols]\n",
    "target_ns = train[['sig_id']+target_nonsc_cols2]\n",
    "\n",
    "train0 = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "# drug ids\n",
    "tar_sig = target['sig_id'].tolist()\n",
    "train_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\n",
    "target = target.merge(train_drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_drug.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 19].index\n",
    "vc2 = vc.loc[vc > 19].index\n",
    "\n",
    "feature_cols = []\n",
    "for key_i in feat_dic.keys():\n",
    "    value_i = feat_dic[key_i]\n",
    "    print(key_i,len(value_i))\n",
    "    feature_cols += value_i\n",
    "feature_cols0 = dp(feature_cols)\n",
    "    \n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "for seed in SEED:\n",
    "    seed_everything(seed=seed)\n",
    "    folds = train0.copy()\n",
    "    feature_cols = dp(feature_cols0)\n",
    "    \n",
    "    # Kfold - leave drug out\n",
    "    target2 = target.copy()\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    target2['kfold'] = target2.drug_id.map(dct1)\n",
    "    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n",
    "    target2.kfold = target2.kfold.astype(int)\n",
    "\n",
    "    folds['kfold'] = target2['kfold'].copy()\n",
    "\n",
    "    train = folds.copy()\n",
    "    test_ = test.copy()\n",
    "\n",
    "    # HyperParameters\n",
    "    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    EPOCHS = 25\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    NFOLDS = 5\n",
    "    EARLY_STOPPING_STEPS = 10\n",
    "    EARLY_STOP = False\n",
    "\n",
    "    n_comp1 = 50\n",
    "    n_comp2 = 15\n",
    "\n",
    "    num_features=len(feature_cols) + n_comp1 + n_comp2\n",
    "    num_targets=len(target_cols)\n",
    "    num_targets_0=len(target_nonsc_cols2)\n",
    "    hidden_size=4096\n",
    "\n",
    "    tar_freq = np.array([np.min(list(f_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
    "    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
    "    tar_weight0_min = dp(np.min(tar_weight0))\n",
    "    tar_weight = tar_weight0_min/tar_weight0\n",
    "    np.mean(tar_weight)\n",
    "    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
    "    \n",
    "    class SmoothBCEwLogits(_WeightedLoss):\n",
    "        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "            super().__init__(weight=weight, reduction=reduction)\n",
    "            self.smoothing = smoothing\n",
    "            self.weight = weight\n",
    "            self.reduction = reduction\n",
    "\n",
    "        @staticmethod\n",
    "        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "            assert 0 <= smoothing < 1\n",
    "            with torch.no_grad():\n",
    "                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            return targets\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                self.smoothing)\n",
    "            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
    "                                                      pos_weight = pos_weight)\n",
    "\n",
    "            if  self.reduction == 'sum':\n",
    "                loss = loss.sum()\n",
    "            elif  self.reduction == 'mean':\n",
    "                loss = loss.mean()\n",
    "\n",
    "            return loss\n",
    "\n",
    "    class TrainDataset:\n",
    "        def __init__(self, features, targets):\n",
    "            self.features = features\n",
    "            self.targets = targets\n",
    "\n",
    "        def __len__(self):\n",
    "            return (self.features.shape[0])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dct = {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "            }\n",
    "            return dct\n",
    "\n",
    "    class TestDataset:\n",
    "        def __init__(self, features):\n",
    "            self.features = features\n",
    "\n",
    "        def __len__(self):\n",
    "            return (self.features.shape[0])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dct = {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "            }\n",
    "            return dct\n",
    "\n",
    "\n",
    "    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "        model.train()\n",
    "        final_loss = 0\n",
    "\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            final_loss += loss.item()\n",
    "\n",
    "        final_loss /= len(dataloader)\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "\n",
    "    def valid_fn(model, loss_fn, dataloader, device):\n",
    "        model.eval()\n",
    "        final_loss = 0\n",
    "        valid_preds = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            final_loss += loss.item()\n",
    "            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        final_loss /= len(dataloader)\n",
    "        valid_preds = np.concatenate(valid_preds)\n",
    "\n",
    "        return final_loss, valid_preds\n",
    "\n",
    "    def inference_fn(model, dataloader, device):\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs = data['x'].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    class Model(nn.Module):\n",
    "\n",
    "        def __init__(self, num_features, num_targets, hidden_size):\n",
    "            super(Model, self).__init__()\n",
    "            cha_1 = 256\n",
    "            cha_2 = 512\n",
    "            cha_3 = 512\n",
    "\n",
    "            cha_1_reshape = int(hidden_size/cha_1)\n",
    "            cha_po_1 = int(hidden_size/cha_1/2)\n",
    "            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "            self.cha_1 = cha_1\n",
    "            self.cha_2 = cha_2\n",
    "            self.cha_3 = cha_3\n",
    "            self.cha_1_reshape = cha_1_reshape\n",
    "            self.cha_po_1 = cha_po_1\n",
    "            self.cha_po_2 = cha_po_2\n",
    "\n",
    "            self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "            self.dropout1 = nn.Dropout(0.1)\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "            self.dropout_c1 = nn.Dropout(0.1)\n",
    "            self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2 = nn.Dropout(0.1)\n",
    "            self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2_1 = nn.Dropout(0.3)\n",
    "            self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2_2 = nn.Dropout(0.2)\n",
    "            self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
    "\n",
    "            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            self.flt = nn.Flatten()\n",
    "\n",
    "            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "            self.dropout3 = nn.Dropout(0.2)\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            x = self.batch_norm1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "            x = x.reshape(x.shape[0],self.cha_1,\n",
    "                          self.cha_1_reshape)\n",
    "\n",
    "            x = self.batch_norm_c1(x)\n",
    "            x = self.dropout_c1(x)\n",
    "            x = F.relu(self.conv1(x))\n",
    "\n",
    "            x = self.ave_po_c1(x)\n",
    "\n",
    "            x = self.batch_norm_c2(x)\n",
    "            x = self.dropout_c2(x)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_s = x\n",
    "\n",
    "            x = self.batch_norm_c2_1(x)\n",
    "            x = self.dropout_c2_1(x)\n",
    "            x = F.relu(self.conv2_1(x))\n",
    "\n",
    "            x = self.batch_norm_c2_2(x)\n",
    "            x = self.dropout_c2_2(x)\n",
    "            x = F.relu(self.conv2_2(x))\n",
    "            x =  x * x_s\n",
    "\n",
    "            x = self.max_po_c2(x)\n",
    "\n",
    "            x = self.flt(x)\n",
    "\n",
    "            x = self.batch_norm3(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.dense3(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def run_training(fold, seed):\n",
    "\n",
    "        seed_everything(seed)\n",
    "\n",
    "        trn_idx = train[train['kfold'] != fold].index\n",
    "        val_idx = train[train['kfold'] == fold].index\n",
    "\n",
    "        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n",
    "        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n",
    "\n",
    "        x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\n",
    "        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n",
    "        x_test = test_[feature_cols]\n",
    "\n",
    "        #------------ norm --------------\n",
    "        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n",
    "        col_num.sort()\n",
    "        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n",
    "        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n",
    "        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n",
    "\n",
    "        #------------ pca --------------\n",
    "        def pca_pre(tr,va,te,\n",
    "                    n_comp,feat_raw,feat_new):\n",
    "            pca = PCA(n_components=n_comp, random_state=42)\n",
    "            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n",
    "            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n",
    "            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n",
    "            return(tr2,va2,te2)\n",
    "\n",
    "\n",
    "        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n",
    "        feat_dic['pca_g'] = pca_feat_g\n",
    "        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n",
    "\n",
    "        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n",
    "        feat_dic['pca_c'] = pca_feat_g\n",
    "        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n",
    "\n",
    "        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n",
    "        \n",
    "        train_dataset = TrainDataset(x_train, y_train)\n",
    "        valid_dataset = TrainDataset(x_valid, y_valid)\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = Model(\n",
    "            num_features=num_features,\n",
    "            num_targets=num_targets,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n",
    "        loss_va = nn.BCEWithLogitsLoss()    \n",
    "\n",
    "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "        early_step = 0\n",
    "\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        best_loss = np.inf\n",
    "\n",
    "        mod_name = mod_path1 + f\"FOLD_mod11_{seed}_{fold}_.pth\"\n",
    "        \n",
    "        model.load_state_dict(torch.load(mod_name, map_location=torch.device('cpu')))\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        oof[val_idx] = inference_fn(model, validloader, DEVICE)\n",
    "        \n",
    "        #--------------------- PREDICTION---------------------\n",
    "        testdataset = TestDataset(x_test)\n",
    "        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        predictions = np.zeros((len(test_), len(target_cols)))\n",
    "        predictions = inference_fn(model, testloader, DEVICE)\n",
    "        return oof, predictions\n",
    "\n",
    "    def run_k_fold(NFOLDS, seed):\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "        for fold in range(NFOLDS):\n",
    "            oof_, pred_ = run_training(fold, seed)\n",
    "\n",
    "            predictions += pred_ / NFOLDS\n",
    "            oof += oof_\n",
    "\n",
    "        return oof, predictions\n",
    "\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "    \n",
    "    oof_tmp = dp(oof)\n",
    "    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n",
    "    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n",
    "\n",
    "print(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n",
    "\n",
    "train0[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "### mod1 ###\n",
    "train0_1 = train0.copy()\n",
    "sub_1 = sub.copy()\n",
    "\n",
    "pd.DataFrame(sc_dic,index=['sc']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod2: TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [100,101,102,103,104,105,106,107,108,109]\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "sc_dic = {}\n",
    "feat_dic = {}\n",
    "train_features = pd.read_csv(input_dir+'train_features.csv')\n",
    "train_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv(input_dir+'test_features.csv')\n",
    "sample_submission = pd.read_csv(input_dir+'sample_submission.csv')\n",
    "train_drug = pd.read_csv(input_dir+'train_drug.csv')\n",
    "\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "feat_dic['gene'] = GENES\n",
    "feat_dic['cell'] = CELLS\n",
    "\n",
    "# sample normalization \n",
    "q2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\n",
    "\n",
    "q2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n",
    "\n",
    "q2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "q2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "\n",
    "    feat_raw = train.columns\n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "\n",
    "        df['c52_c42'] = df['c-52'] * df['c-42']\n",
    "        df['c13_c73'] = df['c-13'] * df['c-73']\n",
    "        df['c26_c13'] = df['c-23'] * df['c-13']\n",
    "        df['c33_c6'] = df['c-33'] * df['c-6']\n",
    "        df['c11_c55'] = df['c-11'] * df['c-55']\n",
    "        df['c38_c63'] = df['c-38'] * df['c-63']\n",
    "        df['c38_c94'] = df['c-38'] * df['c-94']\n",
    "        df['c13_c94'] = df['c-13'] * df['c-94']\n",
    "        df['c4_c52'] = df['c-4'] * df['c-52']\n",
    "        df['c4_c42'] = df['c-4'] * df['c-42']\n",
    "        df['c13_c38'] = df['c-13'] * df['c-38']\n",
    "        df['c55_c2'] = df['c-55'] * df['c-2']\n",
    "        df['c55_c4'] = df['c-55'] * df['c-4']\n",
    "        df['c4_c13'] = df['c-4'] * df['c-13']\n",
    "        df['c82_c42'] = df['c-82'] * df['c-42']\n",
    "        df['c66_c42'] = df['c-66'] * df['c-42']\n",
    "        df['c6_c38'] = df['c-6'] * df['c-38']\n",
    "        df['c2_c13'] = df['c-2'] * df['c-13']\n",
    "        df['c62_c42'] = df['c-62'] * df['c-42']\n",
    "        df['c90_c55'] = df['c-90'] * df['c-55']      \n",
    "\n",
    "    feat_new = train.columns\n",
    "    feat_stat = list(set(feat_new) - set(feat_raw))\n",
    "    feat_stat.sort()\n",
    "    return train, test, feat_stat\n",
    "\n",
    "train_features,test_features, feat_stat=fe_stats(train_features,test_features)\n",
    "feat_dic['stat'] = feat_stat\n",
    "\n",
    "# remove ctr\n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_scored.columns]\n",
    "\n",
    "train0 = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "\n",
    "\n",
    "# drug ids\n",
    "tar_sig = target['sig_id'].tolist()\n",
    "train_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\n",
    "target = target.merge(train_drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_drug.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 19].index\n",
    "vc2 = vc.loc[vc > 19].index\n",
    "\n",
    "feature_cols = []\n",
    "for key_i in feat_dic.keys():\n",
    "    value_i = feat_dic[key_i]\n",
    "    print(key_i,len(value_i))\n",
    "    feature_cols += value_i\n",
    "len(feature_cols)\n",
    "feature_cols0 = dp(feature_cols)\n",
    "\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "for seed in SEED:\n",
    "    \n",
    "    seed_everything(seed=seed)\n",
    "    folds = train0.copy()\n",
    "    feature_cols = dp(feature_cols0)\n",
    "    \n",
    "    # Kfold - leave drug out\n",
    "    target2 = target.copy()\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    target2['kfold'] = target2.drug_id.map(dct1)\n",
    "    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n",
    "    target2.kfold = target2.kfold.astype(int)\n",
    "\n",
    "    folds['kfold'] = target2['kfold'].copy()\n",
    "\n",
    "    train = folds.copy()\n",
    "    test_ = test.copy()\n",
    "\n",
    "    # HyperParameters\n",
    "    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    NFOLDS = 5\n",
    "\n",
    "    n_comp1 = 600\n",
    "    n_comp2 = 50\n",
    " \n",
    "    tar_freq = np.array([np.min(list(f_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
    "    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
    "    tar_weight0_min = dp(np.min(tar_weight0))\n",
    "    tar_weight = tar_weight0_min/tar_weight0\n",
    "    np.mean(tar_weight)\n",
    "    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
    "    \n",
    "    wgt_bce = dp(F.binary_cross_entropy_with_logits)\n",
    "    wgt_bce.__defaults__ = (None, None, None, 'mean', pos_weight)\n",
    "    \n",
    "    class SmoothBCEwLogits(_WeightedLoss):\n",
    "        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "            super().__init__(weight=weight, reduction=reduction)\n",
    "            self.smoothing = smoothing\n",
    "            self.weight = weight\n",
    "            self.reduction = reduction\n",
    "\n",
    "        @staticmethod\n",
    "        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "            assert 0 <= smoothing < 1\n",
    "            with torch.no_grad():\n",
    "                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            return targets\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                self.smoothing)\n",
    "            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
    "                                                      pos_weight = pos_weight)\n",
    "            if  self.reduction == 'sum':\n",
    "                loss = loss.sum()\n",
    "            elif  self.reduction == 'mean':\n",
    "                loss = loss.mean()\n",
    "\n",
    "            return loss\n",
    "            \n",
    "    def run_training(fold, seed):\n",
    "\n",
    "        seed_everything(seed)\n",
    "\n",
    "        trn_idx = train[train['kfold'] != fold].index\n",
    "        val_idx = train[train['kfold'] == fold].index\n",
    "\n",
    "        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n",
    "        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n",
    "\n",
    "        x_train, y_train  = train_df[feature_cols], train_df[target_cols].values\n",
    "        x_valid, y_valid =  valid_df[feature_cols], valid_df[target_cols].values\n",
    "        x_test = test_[feature_cols]\n",
    "\n",
    "        #------------ norm --------------\n",
    "        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n",
    "        col_num.sort()\n",
    "        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n",
    "        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n",
    "        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n",
    "\n",
    "        #------------ pca --------------\n",
    "        def pca_pre(tr,va,te,\n",
    "                    n_comp,feat_raw,feat_new):\n",
    "            pca = PCA(n_components=n_comp, random_state=42)\n",
    "            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n",
    "            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n",
    "            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n",
    "            return(tr2,va2,te2)\n",
    "\n",
    "        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n",
    "        feat_dic['pca_g'] = pca_feat_g\n",
    "        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n",
    "\n",
    "        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n",
    "        feat_dic['pca_c'] = pca_feat_g\n",
    "        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n",
    "\n",
    "        #------------ var --------------\n",
    "        var_thresh = VarianceThreshold(0.8)\n",
    "        var_thresh.fit(x_train)\n",
    "        x_train = x_train.loc[:,var_thresh.variances_ > 0.8]\n",
    "        x_valid = x_valid.loc[:,var_thresh.variances_ > 0.8]\n",
    "        x_test  = x_test.loc[:,var_thresh.variances_ > 0.8]\n",
    "\n",
    "        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n",
    "\n",
    "        class LogitsLogLoss(Metric):\n",
    "            \"\"\"\n",
    "            LogLoss with sigmoid applied\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                self._name = \"logits_ll\"\n",
    "                self._maximize = False\n",
    "\n",
    "            def __call__(self, y_true, y_pred):\n",
    "                \"\"\"\n",
    "                Compute LogLoss of predictions.\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                y_true: np.ndarray\n",
    "                    Target matrix or vector\n",
    "                y_score: np.ndarray\n",
    "                    Score matrix or vector\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                    float\n",
    "                    LogLoss of predictions vs targets.\n",
    "                \"\"\"\n",
    "                logits = 1 / (1 + np.exp(-y_pred))\n",
    "                aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
    "                return np.mean(-aux)\n",
    "\n",
    "        MAX_EPOCH = 120\n",
    "        # n_d and n_a are different from the original work, 32 instead of 24\n",
    "        # This is the first change in the code from the original\n",
    "        tabnet_params = dict(\n",
    "            n_d = 64,\n",
    "            n_a = 128,\n",
    "            n_steps = 1,\n",
    "            gamma = 1.3,\n",
    "            lambda_sparse = 0,\n",
    "            n_independent = 2,\n",
    "            n_shared = 1,\n",
    "            optimizer_fn = optim.Adam,\n",
    "            optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n",
    "            mask_type = \"entmax\",\n",
    "            scheduler_params = dict(\n",
    "                mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "            scheduler_fn = ReduceLROnPlateau,\n",
    "            seed = seed,\n",
    "            verbose = 10\n",
    "        )\n",
    "\n",
    "        mod_path = mod_path2 + f\"mod21_{seed}_{fold}_.pth.zip\"\n",
    "        model =  TabNetRegressor()\n",
    "        model.load_model(mod_path)\n",
    "\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        valid_preds = 1 / (1 + np.exp(-model.predict(x_valid)))\n",
    "        oof[val_idx] = valid_preds\n",
    "        predictions = 1 / (1 + np.exp(-model.predict(x_test)))\n",
    "        \n",
    "        return oof, predictions\n",
    "\n",
    "    def run_k_fold(NFOLDS, seed):\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "        for fold in range(NFOLDS):\n",
    "            oof_, pred_ = run_training(fold, seed)\n",
    "\n",
    "            predictions += pred_ / NFOLDS\n",
    "            oof += oof_\n",
    "\n",
    "        return oof, predictions\n",
    "\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "    \n",
    "    oof_tmp = dp(oof)\n",
    "    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n",
    "    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n",
    "\n",
    "print(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n",
    "\n",
    "train0[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "### mod2 ###\n",
    "train0_2 = train0.copy()\n",
    "sub_2 = sub.copy()\n",
    "\n",
    "pd.DataFrame(sc_dic,index=['sc']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod3: DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [200, 201, 202, 203 ,204, 205, 206, 207, 208, 209]\n",
    "\n",
    "NFOLDS = 7\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "sc_dic = {}\n",
    "feat_dic = {}\n",
    "train_features = pd.read_csv(input_dir+'train_features.csv')\n",
    "train_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv(input_dir+'test_features.csv')\n",
    "sample_submission = pd.read_csv(input_dir+'sample_submission.csv')\n",
    "train_drug = pd.read_csv(input_dir+'train_drug.csv')\n",
    "\n",
    "target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "aux_target_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "all_target_cols = target_cols + aux_target_cols\n",
    "\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "feat_dic['gene'] = GENES\n",
    "feat_dic['cell'] = CELLS\n",
    "\n",
    "## sample normalization ##\n",
    "q2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\n",
    "q2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n",
    "\n",
    "q2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "train_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "q2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\n",
    "q7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\n",
    "qmean = (q2+q7)/2\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\n",
    "qmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\n",
    "test_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n",
    "\n",
    "# remove ctr\n",
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_scored.columns]\n",
    "\n",
    "train0 = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)\n",
    "\n",
    "# drug ids\n",
    "tar_sig = target['sig_id'].tolist()\n",
    "train_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\n",
    "target = target.merge(train_drug, on='sig_id', how='left') \n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = train_drug.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc <= 19].index\n",
    "vc2 = vc.loc[vc > 19].index\n",
    "\n",
    "feature_cols = []\n",
    "for key_i in feat_dic.keys():\n",
    "    value_i = feat_dic[key_i]\n",
    "    print(key_i,len(value_i))\n",
    "    feature_cols += value_i\n",
    "len(feature_cols)\n",
    "feature_cols0 = dp(feature_cols)\n",
    "    \n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "for seed in SEED:\n",
    "\n",
    "    seed_everything(seed=seed)\n",
    "    folds = train0.copy()\n",
    "    feature_cols = dp(feature_cols0)\n",
    "    \n",
    "    # Kfold - leave drug out\n",
    "    target2 = target.copy()\n",
    "    dct1 = {}; dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits = NFOLDS) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    skf = MultilabelStratifiedKFold(n_splits = NFOLDS) # , shuffle = True, random_state = seed\n",
    "    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n",
    "    tmp_idx = tmp.index.tolist()\n",
    "    tmp_idx.sort()\n",
    "    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n",
    "    tmp = tmp.loc[tmp_idx2]\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    target2['kfold'] = target2.drug_id.map(dct1)\n",
    "    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n",
    "    target2.kfold = target2.kfold.astype(int)\n",
    "\n",
    "    folds['kfold'] = target2['kfold'].copy()\n",
    "\n",
    "    train = folds.copy()\n",
    "    test_ = test.copy()\n",
    "\n",
    "    # HyperParameters\n",
    "    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    EPOCHS = 24\n",
    "    BATCH_SIZE = 128\n",
    "\n",
    "    WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n",
    "    MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n",
    "    DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n",
    "    PCT_START = 0.1\n",
    "\n",
    "    n_comp1 = 600\n",
    "    n_comp2 = 50\n",
    "\n",
    "    num_targets = len(target_cols)\n",
    "    num_aux_targets = len(aux_target_cols)\n",
    "    num_all_targets = len(all_target_cols)\n",
    "    hidden_size=4096\n",
    "\n",
    "    tar_freq = np.array([np.min(list(f_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
    "    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
    "    tar_weight0_min = dp(np.min(tar_weight0))\n",
    "    tar_weight = tar_weight0_min/tar_weight0\n",
    "    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
    "\n",
    "    tar_freq = np.array([np.min(list(f_table(train[all_target_cols].iloc[:,i]).values())) for i in range(len(all_target_cols))])\n",
    "    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
    "    tar_weight0_min = dp(np.min(tar_weight0))\n",
    "    pos_weight_all = tar_weight0_min/tar_weight0\n",
    "    pos_weight_all = torch.tensor(pos_weight_all).to(DEVICE)\n",
    "\n",
    "    class SmoothBCEwLogits(_WeightedLoss):\n",
    "        def __init__(self, weight=None, reduction='mean', smoothing=0.0,pos_weight = None):\n",
    "            super().__init__(weight=weight, reduction=reduction)\n",
    "            self.smoothing = smoothing\n",
    "            self.weight = weight\n",
    "            self.reduction = reduction\n",
    "            self.pos_weight = pos_weight\n",
    "\n",
    "        @staticmethod\n",
    "        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "            assert 0 <= smoothing < 1\n",
    "            with torch.no_grad():\n",
    "                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            return targets\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                self.smoothing)\n",
    "            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
    "                                                      pos_weight = self.pos_weight)\n",
    "\n",
    "            if  self.reduction == 'sum':\n",
    "                loss = loss.sum()\n",
    "            elif  self.reduction == 'mean':\n",
    "                loss = loss.mean()\n",
    "\n",
    "            return loss\n",
    "\n",
    "    class TrainDataset:\n",
    "        def __init__(self, features, targets):\n",
    "            self.features = features\n",
    "            self.targets = targets\n",
    "\n",
    "        def __len__(self):\n",
    "            return (self.features.shape[0])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dct = {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "            }\n",
    "            return dct\n",
    "\n",
    "    class TestDataset:\n",
    "        def __init__(self, features):\n",
    "            self.features = features\n",
    "\n",
    "        def __len__(self):\n",
    "            return (self.features.shape[0])\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            dct = {\n",
    "                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "            }\n",
    "            return dct\n",
    "\n",
    "\n",
    "    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "        model.train()\n",
    "        final_loss = 0\n",
    "\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            final_loss += loss.item()\n",
    "\n",
    "        final_loss /= len(dataloader)\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "    def valid_fn(model, loss_fn, dataloader, device):\n",
    "        model.eval()\n",
    "        final_loss = 0\n",
    "        valid_preds = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            final_loss += loss.item()\n",
    "            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        final_loss /= len(dataloader)\n",
    "        valid_preds = np.concatenate(valid_preds)\n",
    "\n",
    "        return final_loss, valid_preds\n",
    "\n",
    "    def inference_fn(model, dataloader, device):\n",
    "        model.eval()\n",
    "        preds = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs = data['x'].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self, num_features, num_targets):\n",
    "            super(Model, self).__init__()\n",
    "            self.hidden_size = [1500, 1250, 1000, 750]\n",
    "            self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n",
    "\n",
    "            self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "            self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n",
    "\n",
    "            self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n",
    "            self.dropout2 = nn.Dropout(self.dropout_value[0])\n",
    "            self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n",
    "\n",
    "            self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n",
    "            self.dropout3 = nn.Dropout(self.dropout_value[1])\n",
    "            self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n",
    "\n",
    "            self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n",
    "            self.dropout4 = nn.Dropout(self.dropout_value[2])\n",
    "            self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n",
    "\n",
    "            self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n",
    "            self.dropout5 = nn.Dropout(self.dropout_value[3])\n",
    "            self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.batch_norm1(x)\n",
    "            x = F.leaky_relu(self.dense1(x))\n",
    "\n",
    "            x = self.batch_norm2(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = F.leaky_relu(self.dense2(x))\n",
    "\n",
    "            x = self.batch_norm3(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = F.leaky_relu(self.dense3(x))\n",
    "\n",
    "            x = self.batch_norm4(x)\n",
    "            x = self.dropout4(x)\n",
    "            x = F.leaky_relu(self.dense4(x))\n",
    "\n",
    "            x = self.batch_norm5(x)\n",
    "            x = self.dropout5(x)\n",
    "            x = self.dense5(x)\n",
    "            return x\n",
    "        \n",
    "    class FineTuneScheduler:\n",
    "        def __init__(self, epochs):\n",
    "            self.epochs = epochs\n",
    "            self.epochs_per_step = 0\n",
    "            self.frozen_layers = []\n",
    "\n",
    "        def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n",
    "            self.frozen_layers = []\n",
    "\n",
    "            model_new = Model(num_features, num_targets)\n",
    "            model_new.load_state_dict(model.state_dict())\n",
    "\n",
    "            # Freeze all weights\n",
    "            for name, param in model_new.named_parameters():\n",
    "                layer_index = name.split('.')[0][-1]\n",
    "\n",
    "                if layer_index == '5':\n",
    "                    continue\n",
    "\n",
    "                param.requires_grad = False\n",
    "\n",
    "                # Save frozen layer names\n",
    "                if layer_index not in self.frozen_layers:\n",
    "                    self.frozen_layers.append(layer_index)\n",
    "\n",
    "            self.epochs_per_step = self.epochs // len(self.frozen_layers)\n",
    "\n",
    "            # Replace the top layers with another ones\n",
    "            model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n",
    "            model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n",
    "            model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n",
    "            model_new.to(DEVICE)\n",
    "            return model_new\n",
    "\n",
    "        def step(self, epoch, model):\n",
    "            if len(self.frozen_layers) == 0:\n",
    "                return\n",
    "\n",
    "            if epoch % self.epochs_per_step == 0:\n",
    "                last_frozen_index = self.frozen_layers[-1]\n",
    "\n",
    "                # Unfreeze parameters of the last frozen layer\n",
    "                for name, param in model.named_parameters():\n",
    "                    layer_index = name.split('.')[0][-1]\n",
    "\n",
    "                    if layer_index == last_frozen_index:\n",
    "                        param.requires_grad = True\n",
    "\n",
    "                del self.frozen_layers[-1]  # Remove the last layer as unfrozen\n",
    "\n",
    "    def run_training(fold, seed):\n",
    "\n",
    "        seed_everything(seed)\n",
    "\n",
    "        trn_idx = train[train['kfold'] != fold].index\n",
    "        val_idx = train[train['kfold'] == fold].index\n",
    "\n",
    "        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n",
    "        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n",
    "\n",
    "        x_train, y_train, y_train_all  = train_df[feature_cols], train_df[target_cols].values, train_df[all_target_cols].values\n",
    "        x_valid, y_valid, y_valid_all =  valid_df[feature_cols], valid_df[target_cols].values, valid_df[all_target_cols].values\n",
    "        x_test = test_[feature_cols]\n",
    "\n",
    "        #------------ norm --------------\n",
    "        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n",
    "        col_num.sort()\n",
    "        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n",
    "        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n",
    "        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n",
    "\n",
    "        #------------ pca --------------\n",
    "        def pca_pre(tr,va,te,\n",
    "                    n_comp,feat_raw,feat_new):\n",
    "            pca = PCA(n_components=n_comp, random_state=42)\n",
    "            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n",
    "            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n",
    "            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n",
    "            return(tr2,va2,te2)\n",
    "\n",
    "        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n",
    "        feat_dic['pca_g'] = pca_feat_g\n",
    "        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n",
    "\n",
    "        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n",
    "        feat_dic['pca_c'] = pca_feat_g\n",
    "        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n",
    "                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n",
    "        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n",
    "        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n",
    "        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n",
    "\n",
    "        #------------ var --------------\n",
    "        var_thresh = VarianceThreshold(0.8)\n",
    "        var_thresh.fit(x_train)\n",
    "        x_train = x_train.loc[:,var_thresh.variances_ > 0.8]\n",
    "        x_valid = x_valid.loc[:,var_thresh.variances_ > 0.8]\n",
    "        x_test  = x_test.loc[:,var_thresh.variances_  > 0.8]\n",
    "\n",
    "        num_features = x_train.shape[1]\n",
    "\n",
    "        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n",
    "\n",
    "        def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n",
    "            if tag_name == 'ALL_TARGETS':\n",
    "                train_dataset = TrainDataset(x_train, y_train_all)\n",
    "                valid_dataset = TrainDataset(x_valid, y_valid_all)\n",
    "            else:\n",
    "                train_dataset = TrainDataset(x_train, y_train)\n",
    "                valid_dataset = TrainDataset(x_valid, y_valid)\n",
    "\n",
    "            trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n",
    "            scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                      steps_per_epoch=len(trainloader),\n",
    "                                                      pct_start=PCT_START,\n",
    "                                                      div_factor=DIV_FACTOR[tag_name], \n",
    "                                                      max_lr=MAX_LR[tag_name],\n",
    "                                                      epochs=EPOCHS)\n",
    "\n",
    "            if tag_name == 'ALL_TARGETS':\n",
    "                loss_tr = SmoothBCEwLogits(smoothing=0.001,pos_weight = pos_weight_all)\n",
    "            else:\n",
    "                loss_tr = SmoothBCEwLogits(smoothing=0.001,pos_weight = pos_weight)\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            oof = np.zeros((len(train), len(target_cols_now)))\n",
    "            best_loss = np.inf\n",
    "\n",
    "            mod_name = f\"mod31_{tag_name}_{seed}_{fold}.pth\"\n",
    "            for epoch in range(EPOCHS):\n",
    "                if fine_tune_scheduler is not None:\n",
    "                    fine_tune_scheduler.step(epoch, model)\n",
    "                train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n",
    "                valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "                print(f\"SEED: {seed}, FOLD: {fold}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n",
    "                if np.isnan(valid_loss):\n",
    "                    break\n",
    "                if valid_loss < best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    oof[val_idx] = valid_preds\n",
    "                    torch.save(model.state_dict(),mod_name )\n",
    "            return oof\n",
    "\n",
    "        fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n",
    "\n",
    "        tag_name = 'SCORED_ONLY'\n",
    "        mod_name = mod_path3 + f\"mod31_{tag_name}_{seed}_{fold}.pth\"\n",
    "        # Load the fine-tuned model with the best loss\n",
    "        model = Model(num_features, num_targets)\n",
    "        model.load_state_dict(torch.load(mod_name, map_location=torch.device('cpu')))\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        #--------------------- PREDICTION---------------------\n",
    "        valid_dataset = TrainDataset(x_valid, y_valid)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        oof[val_idx] = inference_fn(model, validloader, DEVICE)\n",
    "        \n",
    "        testdataset = TestDataset(x_test)\n",
    "        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        predictions = np.zeros((len(test_), num_targets))\n",
    "        predictions = inference_fn(model, testloader, DEVICE)\n",
    "        return oof, predictions\n",
    "\n",
    "\n",
    "    def run_k_fold(NFOLDS, seed):\n",
    "        oof = np.zeros((len(train), len(target_cols)))\n",
    "        predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "        for fold in range(NFOLDS):\n",
    "            oof_, pred_ = run_training(fold, seed)\n",
    "\n",
    "            predictions += pred_ / NFOLDS\n",
    "            oof += oof_\n",
    "\n",
    "        return oof, predictions\n",
    "\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "    \n",
    "    oof_tmp = dp(oof)\n",
    "    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n",
    "    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n",
    "\n",
    "print(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n",
    "\n",
    "train0[target_cols] = oof\n",
    "test[target_cols] = predictions\n",
    "\n",
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "### mod3 ###\n",
    "train0_3 = train0.copy()\n",
    "sub_3 = sub.copy()\n",
    "\n",
    "pd.DataFrame(sc_dic,index=['sc']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 models weighted average\n",
    "sub = sub_1.copy()\n",
    "sub[target_cols] = sub_1[target_cols] * 0.65 + sub_2[target_cols] * 0.1 + sub_3[target_cols] * 0.25\n",
    "\n",
    "# final submission\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
