{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative-stratification/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nfrom copy import deepcopy as dp\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef norm_fit(df_1,saveM = True, sc_name = 'zsco'):   \n    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n    ss_1_dic = {'zsco':StandardScaler(),\n                'mima':MinMaxScaler(),\n                'maxb':MaxAbsScaler(), \n                'robu':RobustScaler(),\n                'norm':Normalizer(), \n                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n                'powe':PowerTransformer()}\n    ss_1 = ss_1_dic[sc_name]\n    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n    if saveM == False:\n        return(df_2)\n    else:\n        return(df_2,ss_1)\n\ndef norm_tra(df_1,ss_x):\n    df_2 = pd.DataFrame(ss_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n    return(df_2)\n\ndef g_table(list1):\n    table_dic = {}\n    for i in list1:\n        if i not in table_dic.keys():\n            table_dic[i] = 1\n        else:\n            table_dic[i] += 1\n    return(table_dic)\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = [0, 1, 2, 3 ,4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\ninput_dir = '../input/lish-moa/'\n\nsc_dic = {}\nfeat_dic = {}\ntrain_features = pd.read_csv(input_dir+'train_features.csv')\ntrain_targets_scored = pd.read_csv(input_dir+'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(input_dir+'train_targets_nonscored.csv')\ntest_features = pd.read_csv(input_dir+'test_features.csv')\nsample_submission = pd.read_csv(input_dir+'sample_submission.csv')\ntrain_drug = pd.read_csv(input_dir+'train_drug.csv')\n\ntarget_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()\ntarget_nonsc_cols = train_targets_nonscored.drop('sig_id', axis=1).columns.values.tolist()\n\n######## non-score ########\nnonctr_id = train_features.loc[train_features['cp_type']!='ctl_vehicle','sig_id'].tolist()\ntmp_con1 = [i in nonctr_id for i in train_targets_scored['sig_id']]\nmat_cor = pd.DataFrame(np.corrcoef(train_targets_scored.drop('sig_id',axis = 1)[tmp_con1].T,\n                      train_targets_nonscored.drop('sig_id',axis = 1)[tmp_con1].T))\nmat_cor2 = mat_cor.iloc[(train_targets_scored.shape[1]-1):,0:train_targets_scored.shape[1]-1]\nmat_cor2.index = target_nonsc_cols\nmat_cor2.columns = target_cols\nmat_cor2 = mat_cor2.dropna()\nmat_cor2_max = mat_cor2.abs().max(axis = 1)\n\nq_n_cut = 0.9\ntarget_nonsc_cols2 = mat_cor2_max[mat_cor2_max > np.quantile(mat_cor2_max,q_n_cut)].index.tolist()\nprint(len(target_nonsc_cols2))\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfeat_dic['gene'] = GENES\nfeat_dic['cell'] = CELLS\n\n# sample norm \nq2 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = train_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\nqmean = (q2+q7)/2\ntrain_features[feat_dic['gene']] = (train_features[feat_dic['gene']].T - qmean.values).T\nq2 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = test_features[feat_dic['gene']].apply(np.quantile,axis = 1,q = 0.75).copy()\nqmean = (q2+q7)/2\ntest_features[feat_dic['gene']] = (test_features[feat_dic['gene']].T - qmean.values).T\n\nq2 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = train_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\nqmean = (q2+q7)/2\ntrain_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T - qmean.values).T\nqmean2 = train_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\ntrain_features[feat_dic['cell']] = (train_features[feat_dic['cell']].T / qmean2.values).T.copy()\n\nq2 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.25).copy()\nq7 = test_features[feat_dic['cell']].apply(np.quantile,axis = 1,q = 0.72).copy()\nqmean = (q2+q7)/2\ntest_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T - qmean.values).T\nqmean2 = test_features[feat_dic['cell']].abs().apply(np.quantile,axis = 1,q = 0.75).copy()+4\ntest_features[feat_dic['cell']] = (test_features[feat_dic['cell']].T / qmean2.values).T.copy()\n\n# remove ctl\ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored[['sig_id']+target_nonsc_cols2], on='sig_id')\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[['sig_id']+target_cols]\ntarget_ns = train[['sig_id']+target_nonsc_cols2]\n\ntrain0 = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\n# drug ids\ntar_sig = target['sig_id'].tolist()\ntrain_drug = train_drug.loc[[i in tar_sig for i in train_drug['sig_id']]]\ntarget = target.merge(train_drug, on='sig_id', how='left') \n\n# LOCATE DRUGS\nvc = train_drug.drug_id.value_counts()\nvc1 = vc.loc[vc <= 19].index\nvc2 = vc.loc[vc > 19].index\n\nfeature_cols = []\nfor key_i in feat_dic.keys():\n    value_i = feat_dic[key_i]\n    print(key_i,len(value_i))\n    feature_cols += value_i\nlen(feature_cols)\nfeature_cols0 = dp(feature_cols)\n    \noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\n# Averaging on multiple SEEDS\nfor seed in SEED:\n\n    seed_everything(seed=seed)\n    folds = train0.copy()\n    feature_cols = dp(feature_cols0)\n    \n    # kfold - leave drug out\n    target2 = target.copy()\n    dct1 = {}; dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.groupby('drug_id')[target_cols].mean().loc[vc1]\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.index[idxV].values}\n        dct1.update(dd)\n\n    # STRATIFY DRUGS MORE THAN 19X\n    skf = MultilabelStratifiedKFold(n_splits = 5) # , shuffle = True, random_state = seed\n    tmp = target2.loc[target2.drug_id.isin(vc2)].reset_index(drop = True)\n    tmp_idx = tmp.index.tolist()\n    tmp_idx.sort()\n    tmp_idx2 = random.sample(tmp_idx,len(tmp_idx))\n    tmp = tmp.loc[tmp_idx2]\n    for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols])):\n        dd = {k:fold for k in tmp.sig_id[idxV].values}\n        dct2.update(dd)\n\n    target2['kfold'] = target2.drug_id.map(dct1)\n    target2.loc[target2.kfold.isna(),'kfold'] = target2.loc[target2.kfold.isna(),'sig_id'].map(dct2)\n    target2.kfold = target2.kfold.astype(int)\n\n    folds['kfold'] = target2['kfold'].copy()\n\n    train = folds.copy()\n    test_ = test.copy()\n\n    # HyperParameters\n    DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n    EPOCHS = 25\n    BATCH_SIZE = 128\n    LEARNING_RATE = 1e-3\n    WEIGHT_DECAY = 1e-5\n    NFOLDS = 5\n    EARLY_STOPPING_STEPS = 10\n    EARLY_STOP = False\n\n    n_comp1 = 50\n    n_comp2 = 15\n\n    num_features=len(feature_cols) + n_comp1 + n_comp2\n    num_targets=len(target_cols)\n    num_targets_0=len(target_nonsc_cols2)\n    hidden_size=4096\n\n    tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n    tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n    tar_weight0_min = dp(np.min(tar_weight0))\n    tar_weight = tar_weight0_min/tar_weight0\n    pos_weight = torch.tensor(tar_weight).to(DEVICE)\n    from torch.nn.modules.loss import _WeightedLoss\n    class SmoothBCEwLogits(_WeightedLoss):\n        def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n            super().__init__(weight=weight, reduction=reduction)\n            self.smoothing = smoothing\n            self.weight = weight\n            self.reduction = reduction\n\n        @staticmethod\n        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n            assert 0 <= smoothing < 1\n            with torch.no_grad():\n                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            return targets\n\n        def forward(self, inputs, targets):\n            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n                self.smoothing)\n            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n                                                      pos_weight = pos_weight)\n\n            if  self.reduction == 'sum':\n                loss = loss.sum()\n            elif  self.reduction == 'mean':\n                loss = loss.mean()\n\n            return loss\n\n    class TrainDataset:\n        def __init__(self, features, targets):\n            self.features = features\n            self.targets = targets\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n                'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n            }\n            return dct\n\n    class TestDataset:\n        def __init__(self, features):\n            self.features = features\n\n        def __len__(self):\n            return (self.features.shape[0])\n\n        def __getitem__(self, idx):\n            dct = {\n                'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n            }\n            return dct\n\n\n    def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n        model.train()\n        final_loss = 0\n\n        for data in dataloader:\n            optimizer.zero_grad()\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            final_loss += loss.item()\n\n        final_loss /= len(dataloader)\n\n        return final_loss\n\n\n    def valid_fn(model, loss_fn, dataloader, device):\n        model.eval()\n        final_loss = 0\n        valid_preds = []\n\n        for data in dataloader:\n            inputs, targets = data['x'].to(device), data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            final_loss += loss.item()\n            valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        final_loss /= len(dataloader)\n        valid_preds = np.concatenate(valid_preds)\n\n        return final_loss, valid_preds\n\n    def inference_fn(model, dataloader, device):\n        model.eval()\n        preds = []\n\n        for data in dataloader:\n            inputs = data['x'].to(device)\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n        preds = np.concatenate(preds)\n\n        return preds\n\n    class Model(nn.Module):\n        def __init__(self, num_features, num_targets, hidden_size):\n            super(Model, self).__init__()\n            cha_1 = 256\n            cha_2 = 512\n            cha_3 = 512\n\n            cha_1_reshape = int(hidden_size/cha_1)\n            cha_po_1 = int(hidden_size/cha_1/2)\n            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n\n            self.cha_1 = cha_1\n            self.cha_2 = cha_2\n            self.cha_3 = cha_3\n            self.cha_1_reshape = cha_1_reshape\n            self.cha_po_1 = cha_po_1\n            self.cha_po_2 = cha_po_2\n\n            self.batch_norm1 = nn.BatchNorm1d(num_features)\n            self.dropout1 = nn.Dropout(0.1)\n            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n            self.dropout_c1 = nn.Dropout(0.1)\n            self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n\n            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n\n            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2 = nn.Dropout(0.1)\n            self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_1 = nn.Dropout(0.3)\n            self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n\n            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n            self.dropout_c2_2 = nn.Dropout(0.2)\n            self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n\n            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n            self.flt = nn.Flatten()\n\n            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n            self.dropout3 = nn.Dropout(0.2)\n            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n\n        def forward(self, x):\n\n            x = self.batch_norm1(x)\n            x = self.dropout1(x)\n            x = F.celu(self.dense1(x), alpha=0.06)\n\n            x = x.reshape(x.shape[0],self.cha_1,\n                          self.cha_1_reshape)\n\n            x = self.batch_norm_c1(x)\n            x = self.dropout_c1(x)\n            x = F.relu(self.conv1(x))\n\n            x = self.ave_po_c1(x)\n\n            x = self.batch_norm_c2(x)\n            x = self.dropout_c2(x)\n            x = F.relu(self.conv2(x))\n            x_s = x\n\n            x = self.batch_norm_c2_1(x)\n            x = self.dropout_c2_1(x)\n            x = F.relu(self.conv2_1(x))\n\n            x = self.batch_norm_c2_2(x)\n            x = self.dropout_c2_2(x)\n            x = F.relu(self.conv2_2(x))\n            x =  x * x_s\n\n            x = self.max_po_c2(x)\n\n            x = self.flt(x)\n\n            x = self.batch_norm3(x)\n            x = self.dropout3(x)\n            x = self.dense3(x)\n\n            return x\n\n    def run_training(fold, seed):\n\n        seed_everything(seed)\n\n        trn_idx = train[train['kfold'] != fold].index\n        val_idx = train[train['kfold'] == fold].index\n\n        train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n        valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n\n        x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\n        x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n        x_test = test_[feature_cols]\n\n        #------------ norm --------------\n        col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n        col_num.sort()\n        x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n        x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n        x_test[col_num]     = norm_tra(x_test[col_num],ss)\n\n        #------------ pca --------------\n        def pca_pre(tr,va,te,\n                    n_comp,feat_raw,feat_new):\n            pca = PCA(n_components=n_comp, random_state=42)\n            tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n            va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n            te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n            return(tr2,va2,te2)\n\n\n        pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n        feat_dic['pca_g'] = pca_feat_g\n        x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n                                                   n_comp1,feat_dic['gene'],pca_feat_g)\n        x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n        x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n        x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n\n        pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n        feat_dic['pca_c'] = pca_feat_g\n        x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n                                                   n_comp2,feat_dic['cell'],pca_feat_g)\n        x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n        x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n        x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n\n        x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n\n        train_dataset = TrainDataset(x_train, y_train_ns)\n        valid_dataset = TrainDataset(x_valid, y_valid_ns)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets_0,\n            hidden_size=hidden_size,\n        )\n\n        model.to(DEVICE)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, \n                                                  max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n        loss_tr = nn.BCEWithLogitsLoss()   #SmoothBCEwLogits(smoothing = 0.001)\n        loss_va = nn.BCEWithLogitsLoss()    \n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        for epoch in range(1):\n            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n            print(f\"FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n\n        model.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\n        model.to(DEVICE)\n\n        train_dataset = TrainDataset(x_train, y_train)\n        valid_dataset = TrainDataset(x_valid, y_valid)\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n        loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n        loss_va = nn.BCEWithLogitsLoss()    \n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        oof = np.zeros((len(train), len(target_cols)))\n        best_loss = np.inf\n\n        mod_name = f\"FOLD_mod11_{seed}_{fold}_.pth\"\n        \n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n            print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n\n            if valid_loss < best_loss:\n\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), mod_name)\n\n            elif(EARLY_STOP == True):\n\n                early_step += 1\n                if (early_step >= early_stopping_steps):\n                    break\n\n        #--------------------- PREDICTION---------------------\n        testdataset = TestDataset(x_test)\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n        )\n\n        model.load_state_dict(torch.load(mod_name))\n        model.to(DEVICE)\n\n        predictions = np.zeros((len(test_), len(target_cols)))\n        predictions = inference_fn(model, testloader, DEVICE)\n        return oof, predictions\n\n    def run_k_fold(NFOLDS, seed):\n        oof = np.zeros((len(train), len(target_cols)))\n        predictions = np.zeros((len(test), len(target_cols)))\n\n        for fold in range(NFOLDS):\n            oof_, pred_ = run_training(fold, seed)\n\n            predictions += pred_ / NFOLDS\n            oof += oof_\n\n        return oof, predictions\n\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n    \n    oof_tmp = dp(oof)\n    oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n    sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])\n    \n\nfrom sklearn.metrics import log_loss\nprint(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n\ntrain0[target_cols] = oof\ntest[target_cols] = predictions\n\n### for blend test ###\ntrain0.to_csv('train_pred.csv', index=False)\n### for blend test ###\n\nsub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(sc_dic,index=['sc']).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}