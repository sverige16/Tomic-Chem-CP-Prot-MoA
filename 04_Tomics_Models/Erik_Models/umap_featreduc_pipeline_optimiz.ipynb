{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[90]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Import Statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split # Functipn to split data into training, validation and test sets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "import glob   # The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order. No tilde expansion is done, but *, ?, and character ranges expressed with [] will be correctly matched.\n",
    "import os   # miscellneous operating system interfaces. This module provides a portable way of using operating system dependent functionality. If you just want to read or write a file see open(), if you want to manipulate paths, see the os.path module, and if you want to read all the lines in all the files on the command line see the fileinput module.\n",
    "import random       \n",
    "from tqdm import tqdm \n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "import math \n",
    "\n",
    "import umap\n",
    "import math\n",
    "\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_curve,log_loss, accuracy_score, f1_score\n",
    "from sklearn.metrics import average_precision_score,roc_auc_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import os\n",
    "import time\n",
    "from time import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from skmultilearn.adapt import MLkNN\n",
    "\n",
    "# CMAP (extracting relevant transcriptomic profiles)\n",
    "from cmapPy.pandasGEXpress.parse import parse\n",
    "import cmapPy.pandasGEXpress.subset_gctoo as sg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "from sklearn.decomposition import PCA,FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler,QuantileTransformer\n",
    "from sklearn.metrics import precision_recall_curve,log_loss\n",
    "from sklearn.metrics import average_precision_score,roc_auc_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_tabnet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "nn._estimator_type = \"classifier\"\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------# \n",
    "# Prepping Neptune.ai for logging --------------------------------------------------------------------------------------------\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "# Downloading all relevant data frames and csv files ----------------------------------------------------------\n",
    "\n",
    "# clue column metadata with columns representing compounds in common with SPECs 1 & 2\n",
    "clue_sig_in_SPECS = pd.read_csv('/home/jovyan/Tomics-CP-Chem-MoA/04_Tomics_Models/init_data_expl/clue_sig_in_SPECS1&2.csv', delimiter = \",\")\n",
    "\n",
    "# clue row metadata with rows representing transcription levels of specific genes\n",
    "clue_gene = pd.read_csv('/home/jovyan/Tomics-CP-Chem-MoA/04_Tomics_Models/init_data_expl/clue_geneinfo_beta.txt', delimiter = \"\\t\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "def load_train_valid_data(train_data, valid_data):\n",
    "    '''\n",
    "    Functions loads the data frames that will be used to train classifier and assess its accuracy in predicting.\n",
    "    input:\n",
    "        train_data: filename of training csv file\n",
    "        valid_data: filename of validation csv file\n",
    "    ouput:\n",
    "       L1000 training: pandas dataframe with training data\n",
    "       L1000 validation: pandas dataframe with validation data\n",
    "    '''\n",
    "    path = '/home/jovyan/Tomics-CP-Chem-MoA/04_Tomics_Models/data_split_csvs/'\n",
    "    L1000_training = pd.read_csv(path + train_data, delimiter = \",\")\n",
    "    L1000_validation =pd.read_csv(path + valid_data, delimiter = \",\")\n",
    "    return L1000_training, L1000_validation\n",
    "\n",
    "def variance_threshold(x_train, x_val):\n",
    "    \"\"\"\n",
    "    This function perform feature selection on the data, i.e. removes all low-variance features below the\n",
    "    given 'threshold' parameter.\n",
    "    \n",
    "    Args:\n",
    "            x_fold_train: K-fold train data with only phenotypic/morphological features and PCs - pandas \n",
    "            dataframe.\n",
    "            x_fold_val: K-fold validation data with only phenotypic/morphological features and PCs - pandas \n",
    "            dataframe.\n",
    "            df_test_x_copy: test data - pandas dataframe with only phenotypic/morphological features and PCs.\n",
    "    \n",
    "    Returns:\n",
    "            x_fold_train: K-fold train data after feature selection - pandas dataframe.\n",
    "            x_fold_val: K-fold validation data after feature selection - pandas dataframe.\n",
    "            df_test_x_copy: test data - pandas dataframe after feature selection - pandas dataframe.\n",
    "    \n",
    "    inspired by https://github.com/broadinstitute/lincs-profiling-complementarity/tree/master/2.MOA-prediction\n",
    "    \n",
    "    \"\"\"\n",
    "    var_thresh = VarianceThreshold(threshold = 0.8) # sets a variance threshold\n",
    "    var_thresh.fit(x_train) # learn empirical variances from X\n",
    "    x_train = x_train.loc[:,var_thresh.variances_ > 0.8] # locate all variance thresholds above 0.8, keep those columns\n",
    "    x_val = x_val.loc[:,var_thresh.variances_ > 0.8]\n",
    "    return x_train, x_val\n",
    "\n",
    "def normalize_func(trn, test):\n",
    "    \"\"\"\n",
    "    Performs quantile normalization on the train, test and validation data. The QuantileTransformer\n",
    "    is fitted on the train data, and transformed on test and validation data.\n",
    "    \n",
    "    Args:\n",
    "            trn: train data - pandas dataframe.\n",
    "            val: validation data - pandas dataframe.\n",
    "            test: test data - pandas dataframe.\n",
    "    \n",
    "    Returns:\n",
    "            trn_norm: normalized train data - pandas dataframe.\n",
    "            val_norm: normalized validation - pandas dataframe.\n",
    "            test_norm: normalized test data - pandas dataframe.\n",
    "    inspired by  https://github.com/broadinstitute/lincs-profiling-complementarity/tree/master/2.MOA-prediction\n",
    "    \"\"\"\n",
    "    norm_model = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    # norm_model = StandardScaler()\n",
    "    trn_norm = pd.DataFrame(norm_model.fit_transform(trn),index = trn.index,columns = trn.columns)\n",
    "    tst_norm = pd.DataFrame(norm_model.transform(test),index = test.index,columns = test.columns)\n",
    "    return trn_norm, tst_norm\n",
    "\n",
    "\n",
    "def tprofiles_gc_too_func(data, clue_gene):\n",
    "    '''\n",
    "    Function preparing the gctoo dataframe to extract from gctx file, choosing only landmark genes\n",
    "    \n",
    "    Input:\n",
    "    data: column meta data from clue.io that only includes training/test data\n",
    "    clue_gene: row meta data from clue.io transcriptomic profiles\n",
    "    \n",
    "    Output:\n",
    "    parsed gctoo file with all of the transcriptomic profiles. Only landmark genes included.'''\n",
    "\n",
    "    clue_gene[\"gene_id\"] = clue_gene[\"gene_id\"].astype(str)\n",
    "    landmark_gene_row_ids = clue_gene[\"gene_id\"][clue_gene[\"feature_space\"] == \"landmark\"]\n",
    "\n",
    "    # get all samples (across all cell types, doses, and other treatment conditions) with certain MoA\n",
    "    profile_ids = data[\"sig_id\"]\n",
    "    tprofiles_gctoo = parse(\"/scratch2-shared/erikep/level5_beta_trt_cp_n720216x12328.gctx\", \n",
    "                                    cid= profile_ids, \n",
    "                                    rid = landmark_gene_row_ids)\n",
    "\n",
    "    return tprofiles_gctoo\n",
    "\n",
    "def extract_tprofile(profiles_gc_too, idx):\n",
    "    '''returns transcriptomic profile of of specific ID with in the form of a numpy array\n",
    "    \n",
    "    input:\n",
    "     profiles_gc_too: gc_too dataframe hosting transcriptomic profiles\n",
    "     idx:  extract unique column name from L1000 data\n",
    "    \n",
    "    output: \n",
    "      numpy array of a single transcriptomic profile\n",
    "    '''\n",
    "    tprofile_id =  profiles_gc_too.col_metadata_df.iloc[idx]\n",
    "    tprofile_id_sig = [tprofile_id.name] \n",
    "    tprofile_gctoo = sg.subset_gctoo(profiles_gc_too, cid= tprofile_id_sig) \n",
    "    #return torch.tensor(tprofile_gctoo.data_df.values.astype(np.float32)) \n",
    "    return tprofile_id_sig, np.asarray(tprofile_gctoo.data_df.values.astype(np.float32))    \n",
    "\n",
    "\n",
    "def splitting(df):\n",
    "    '''Splitting data into two parts:\n",
    "    1. input : the pointer showing where the transcriptomic profile is  \n",
    "    2. target : labels (the correct MoA)\n",
    "    \n",
    "    Input:\n",
    "        df: pandas dataframe with all columns.\n",
    "    Output:\n",
    "      input: pandas dataframe with all of the features\n",
    "      target : returns the MoA class column separately, and as a string \n",
    "      '''\n",
    "    \n",
    "    target = df['moa']\n",
    "    target = target.apply(str)\n",
    "    input =  df.drop('moa', axis = 1)\n",
    "    \n",
    "    return input, target\n",
    "\n",
    "\n",
    "def np_array_transform(profiles_gc_too):\n",
    "    '''\n",
    "    Takes a .gctoo and extracts the correct profile, transforms the profile into a numpy array and then places it into a pandas data_frame.\n",
    "\n",
    "    Input:\n",
    "        profiles_gc_too: the gc_too dataframe with all the transcriptomic profiles\n",
    "\n",
    "    Output:\n",
    "        df: pandas dataframe, where each row is a transcriptomic profile\n",
    "    '''\n",
    "    rows = []\n",
    "    sig_id_check = []\n",
    "    for i in range(profiles_gc_too.data_df.shape[1]):\n",
    "        sig_id_row, np_array = extract_tprofile(profiles_gc_too, i)\n",
    "        rows.append(np_array)\n",
    "        sig_id_check.append(sig_id_row)\n",
    "    np_array =  np.asarray(rows)\n",
    "    np_array = np_array.squeeze()\n",
    "    df = pd.DataFrame(np_array)\n",
    "    sig_id_df = pd.DataFrame(sig_id_check)\n",
    "    df[\"sig_id\"] =  sig_id_df[0:]\n",
    "    return df\n",
    "\n",
    "def acquire_npy(dataset):\n",
    "    '''\n",
    "    Acquiring the numpy dataset in the npy format if it has already been created. Purpose is to save the reloading of the .npy dataframe, which can take \n",
    "    up to 9 minutes for the 10 MoAs.\n",
    "\n",
    "    Input: \n",
    "    String with either \"train\" or \"val\". Then the user than manually inputs the name of the file.\n",
    "\n",
    "    Ouput:\n",
    "    Returns pandas dataframe from the .npy file found in '/scratch2-shared' given by the user.\n",
    "    '''\n",
    "    path = '/scratch2-shared/erikep/data_splits_npy'\n",
    "    if dataset == 'train':\n",
    "        #filename = input('Give name of npy file (str): ')\n",
    "        #npy_set = np.load(path + filename)\n",
    "        npy_set = np.load('/scratch2-shared/erikep/data_splits_npy/data_splits_npy2_moas_train.npy')\n",
    "    elif dataset == 'val':\n",
    "        #filename = input('Give name of npy file (str): ')\n",
    "        #npy_set = np.load(path + filename)\n",
    "        npy_set = np.load('/scratch2-shared/erikep/data_splits_npy/data_splits_npy2_moas_valid.npy')\n",
    "    else:\n",
    "        filename =  input('Give name of npy file (str): ')\n",
    "        npy_set = np.load(path + filename)\n",
    "    return pd.DataFrame(npy_set)\n",
    "\n",
    "def save_npy(dataset):\n",
    "    '''Save the numpy array of the selected transcriptomics profiles\n",
    "    Input:\n",
    "        dataset: the numpy array to be saved\n",
    "    '''\n",
    "    path = '/scratch2-shared/erikep/data_splits_npy/'\n",
    "    file_name = input(\"Give filename for numpy array: \")\n",
    "    np.save(path + file_name, dataset)\n",
    "\n",
    "def get_models(class_weight):\n",
    "    '''\n",
    "    Input:\n",
    "        class weight: including or not including class weight.\n",
    "    Output:\n",
    "        A list of tuples, with a str with a descriptor followed by the classifier function)\n",
    "    '''\n",
    "    TNC = TabNetClassifier()\n",
    "    TNC._estimator_type = \"classifier\"\n",
    "    models = list()\n",
    "    models.append(('logreg', LogisticRegression(class_weight = class_weight, solver= \"liblinear\", penalty = \"l2\"))) \n",
    "    #models.append(('RFC',RandomForestClassifier(class_weight= class_weight))) \n",
    "    #models.append(('gradboost', GradientBoostingClassifier()))\n",
    "    #models.append(('Ada', AdaBoostClassifier()))\n",
    "    #models.append(('KNN', KNeighborsClassifier(n_neighbors = 5)))\n",
    "    #models.append(('Bagg',BaggingClassifier()))\n",
    "    #models.append(('Tab', TNC))\n",
    "    return models\n",
    "\n",
    "def printing_results(class_alg, labels_val, predictions): \n",
    "    '''\n",
    "    Printing the results from the \n",
    "    Input:\n",
    "        class_alg: name of the model\n",
    "        labels_val: the correct guesses\n",
    "        predictions: the predictions made by the model\n",
    "    Output:\n",
    "        Printed results of accuracy, F1 score, and confusion matrix\n",
    "    '''\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print(class_alg)\n",
    "    anders = f1_score(labels_val, predictions, labels = [\"0\",\"1\"], average = 'macro')\n",
    "    print(f' Accuracy score: {accuracy_score(labels_val, predictions)}')\n",
    "    print(f' F1 Score: {     anders   }')\n",
    "    print(f' Confusion Matrix: {confusion_matrix(labels_val, predictions)}')\n",
    "    print('----------------------------------------------------------------------')\n",
    "\n",
    "def write_list(a_list, file_type):\n",
    "    with open('/home/jovyan/Tomics-CP-Chem-MoA/04_Tomics_Models/pickles/tabnet_pickles/' + file_type + '.pickle', 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "    print('Done writing binary file')\n",
    "\n",
    "def save_val(a_list, file_type):\n",
    "    with open('/home/jovyan/Tomics-CP-Chem-MoA/04_Tomics_Models/pickles/val_order_pickles/' + file_type + '.pickle', 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "    print('Done writing binary file')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "def main(use_variance_threshold, normalize, L1000_training, L1000_validation, clue_gene, npy_exists, apply_class_weight, ensemble):\n",
    "    '''\n",
    "    Tests a series of ML algorithms after optional pre-processing of the data in order to make predictions on the MoA class based on\n",
    "    chosen transcriptomic profiles. \n",
    "\n",
    "    Input:\n",
    "        use_variance_threshold: True/False (also have to adjust hyperparameter in the function itself depending on normalization.)\n",
    "        normalize: True/False. Whether or not to normalize the data.\n",
    "        L1000_training: Str. Name of the csv file with training rows\n",
    "        L1000_validation: Str. Name of the csv file with validation rows\n",
    "        clue_gene: Row metadata fro the transcriptomic profiles\n",
    "        npy_exists: True/False: whether or not the numpy array with transcriptomic profiles has already been created (can save time if many moas are used.)\n",
    "        apply_class_weight: True/False. Whether to apply class weights for the random forest classifier.\n",
    "        ensemble: True/False. Whether to apply to do an ensemble classifier with a select number of classifiers.\n",
    "    Output:\n",
    "        Prints the accuracy, F1 score and confusion matrix for each of the ML algorithms.\n",
    "        Save unique numpy array. \n",
    "    '''\n",
    "    # shuffling training and validation data\n",
    "    L1000_training = L1000_training.sample(frac = 1, random_state = 1)\n",
    "    L1000_validation = L1000_validation.sample(frac = 1, random_state = 1)\n",
    "    # extracting training transcriptomes\n",
    "    profiles_gc_too_train = tprofiles_gc_too_func(L1000_training, clue_gene)\n",
    "    if npy_exists:\n",
    "        df_train = acquire_npy('train')\n",
    "    else:    \n",
    "        df_train = np_array_transform(profiles_gc_too_train)\n",
    "        #save_npy(df_train)\n",
    "    \n",
    "    #input_df_train, labels_train = splitting(L1000_training) \n",
    "    #print(sig_id_df_check.head(20))\n",
    "    #print(input_df_train.sig_id.head(20))\n",
    "    #assert sig_id_df_check.all() == input_df_train.sig_id.all()\n",
    "    \n",
    "    # create dictionary where moas are associated with a number\n",
    "\n",
    "   \n",
    "    # extracting valid \n",
    "    profiles_gc_too_valid = tprofiles_gc_too_func(L1000_validation, clue_gene)\n",
    "    if npy_exists:\n",
    "        df_val = acquire_npy('val')\n",
    "    else:    \n",
    "        df_val = np_array_transform(profiles_gc_too_valid)\n",
    "       \n",
    "    '''\n",
    "    # to normalize\n",
    "    if normalize:\n",
    "        df_train, df_val = normalize_func(df_train, df_val)\n",
    "    \n",
    "    # applying class weights\n",
    "    if apply_class_weight:\n",
    "        class_weight = \"balanced\"\n",
    "    else:\n",
    "        class_weight = None\n",
    "    \n",
    "    models = get_models(class_weight)\n",
    "    scores = list()\n",
    "    # battery of classifiers\n",
    "    for class_alg in models:\n",
    "        classifier = class_alg[1]\n",
    "        # use variance threshold\n",
    "        if use_variance_threshold:\n",
    "            df_train_vs, df_val_vs = variance_threshold(df_train, df_val)\n",
    "            classifier.fit(df_train_vs.values, labels_train.values)\n",
    "            predictions = classifier.predict(df_val_vs.values)\n",
    "            if class_alg[0] == 'Tab':\n",
    "                save_val(labels_val, 'tab_val')\n",
    "                class_probs= classifier.predict_proba(df_val_vs.values)\n",
    "                write_list(predictions, 'predictions')\n",
    "                write_list(class_probs, 'class_probs')\n",
    "\n",
    "        else:\n",
    "            classifier.fit(df_train.values, labels_train.values)\n",
    "            predictions = classifier.predict(df_val.values)\n",
    "            if class_alg[0] == 'Tab':\n",
    "                save_val(labels_val, 'tab_val')\n",
    "                class_probs = classifier.predict_proba(df_val_vs.values)\n",
    "                write_list(predictions, 'predictions')\n",
    "                write_list(class_probs, 'class_probs')\n",
    "        f1_score_from_model = f1_score(labels_val, predictions, average= \"macro\") \n",
    "        scores.append(f1_score_from_model)\n",
    "        printing_results(class_alg, labels_val, predictions)\n",
    "       \n",
    "\n",
    "\n",
    "    if ensemble:\n",
    "        # 'soft':  predict the class labels based on the predicted probabilities p for classifier \n",
    "        ensemble = VotingClassifier(estimators = models, voting = 'soft', weights = scores)\n",
    "        \n",
    "        if use_variance_threshold:\n",
    "                df_train_vs, df_val_vs = variance_threshold(df_train, df_val)\n",
    "                ensemble.fit(df_train_vs.values, labels_train.values)\n",
    "                predictions = ensemble.predict(df_val_vs.values)\n",
    "\n",
    "        else:\n",
    "            ensemble.fit(df_train.values, labels_train.values)\n",
    "            predictions = ensemble.predict(df_val.values)\n",
    "        \n",
    "        printing_results('ensemble', labels_val, predictions)'''\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    # train_filename = input('Training Data Set Filename: ')\n",
    "    #valid_filename = input('Validation Data Set Filename: ')\n",
    "    #train_filename = 'L1000_training_set_cyclo_adr_2.csv'\n",
    "    #valid_filename = 'L1000_test_set_cyclo_adr_2.csv'\n",
    "    train_filename = 'L1000_training_set_nv_cyc_adr.csv'\n",
    "    valid_filename = 'L1000_test_set_nv_cyc_adr.csv'\n",
    "    #train_filename = 'L1000_training_set.csv'\n",
    "    # valid_filename = 'L1000_valid_set.csv'\n",
    "    \n",
    "    # loading data\n",
    "    L1000_training, L1000_validation =  load_train_valid_data(train_filename, valid_filename)\n",
    "    df_train, df_val = main(use_variance_threshold = False, \n",
    "         normalize= True, \n",
    "         L1000_training = L1000_training, \n",
    "         L1000_validation = L1000_validation, \n",
    "         clue_gene= clue_gene, \n",
    "         npy_exists = False,\n",
    "         apply_class_weight= True,\n",
    "         ensemble = False)\n",
    "\n",
    "\n",
    "\n",
    "moa_dictionary = {}\n",
    "for i,j in enumerate(L1000_training.moa.unique()):\n",
    "    moa_dictionary[j] = i\n",
    "\n",
    "# merging the transcriptomic profiles with the corresponding MoA class using the sig_id as a key\n",
    "df_train = pd.merge(df_train, L1000_training[[\"sig_id\", \"moa\"]], how = \"outer\", on =\"sig_id\")\n",
    "df_val = pd.merge(df_val, L1000_validation[[\"sig_id\", \"moa\"]], how = \"outer\", on =\"sig_id\")\n",
    "# dropping the sig_id column\n",
    "df_train.drop(columns = [\"sig_id\"], inplace = True)\n",
    "df_val.drop(columns = [\"sig_id\"], inplace = True)\n",
    "    # separating the features from the labels\n",
    "df_train_features = df_train[df_train.columns[: -1]]\n",
    "df_val_features = df_val[df_val.columns[: -1]]\n",
    "df_train_labels = df_train[\"moa\"]\n",
    "df_val_labels = df_val[\"moa\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>968</th>\n",
       "      <th>969</th>\n",
       "      <th>970</th>\n",
       "      <th>971</th>\n",
       "      <th>972</th>\n",
       "      <th>973</th>\n",
       "      <th>974</th>\n",
       "      <th>975</th>\n",
       "      <th>976</th>\n",
       "      <th>977</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.7527</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.8541</td>\n",
       "      <td>-0.8805</td>\n",
       "      <td>-0.2432</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>-0.2179</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>-0.6835</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3588</td>\n",
       "      <td>-0.5946</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>-0.6292</td>\n",
       "      <td>-0.6189</td>\n",
       "      <td>0.2204</td>\n",
       "      <td>-0.2688</td>\n",
       "      <td>0.7418</td>\n",
       "      <td>-0.2002</td>\n",
       "      <td>0.9993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0940</td>\n",
       "      <td>0.7389</td>\n",
       "      <td>-0.7892</td>\n",
       "      <td>-0.4282</td>\n",
       "      <td>0.7571</td>\n",
       "      <td>1.4361</td>\n",
       "      <td>-0.5794</td>\n",
       "      <td>-1.0546</td>\n",
       "      <td>-2.0860</td>\n",
       "      <td>-0.8119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.8565</td>\n",
       "      <td>-2.2017</td>\n",
       "      <td>0.2389</td>\n",
       "      <td>-0.4334</td>\n",
       "      <td>-0.6115</td>\n",
       "      <td>-0.1084</td>\n",
       "      <td>-1.3946</td>\n",
       "      <td>0.4816</td>\n",
       "      <td>0.7590</td>\n",
       "      <td>-0.5188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8020</td>\n",
       "      <td>-0.2042</td>\n",
       "      <td>-1.3469</td>\n",
       "      <td>-0.0297</td>\n",
       "      <td>-0.9953</td>\n",
       "      <td>-4.4048</td>\n",
       "      <td>-1.9684</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>-2.0086</td>\n",
       "      <td>1.2412</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.3844</td>\n",
       "      <td>-0.8989</td>\n",
       "      <td>-1.0123</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4691</td>\n",
       "      <td>-0.0898</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>0.1521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0       1       2       3       4       5       6       7       8    \\\n",
       "0  1.7527  0.0000 -0.8541 -0.8805 -0.2432  0.5100 -0.2179  0.3331 -0.6835   \n",
       "1  2.0940  0.7389 -0.7892 -0.4282  0.7571  1.4361 -0.5794 -1.0546 -2.0860   \n",
       "2  0.8020 -0.2042 -1.3469 -0.0297 -0.9953 -4.4048 -1.9684  0.4010 -2.0086   \n",
       "\n",
       "      9    ...     968     969     970     971     972     973     974  \\\n",
       "0  0.3024  ... -0.3588 -0.5946  1.0200 -0.6292 -0.6189  0.2204 -0.2688   \n",
       "1 -0.8119  ... -0.8565 -2.2017  0.2389 -0.4334 -0.6115 -0.1084 -1.3946   \n",
       "2  1.2412  ... -1.3844 -0.8989 -1.0123  0.2576  0.1446  0.0000  0.4691   \n",
       "\n",
       "      975     976     977  \n",
       "0  0.7418 -0.2002  0.9993  \n",
       "1  0.4816  0.7590 -0.5188  \n",
       "2 -0.0898  0.6745  0.1521  \n",
       "\n",
       "[3 rows x 978 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "2902    0.0\n",
       "2903    1.0\n",
       "2904    1.0\n",
       "2905    1.0\n",
       "2906    0.0\n",
       "Name: moa, Length: 2907, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_splitting_into_tensor(df):\n",
    "    '''Splitting data into two parts:\n",
    "    1. input : the pointer showing where the transcriptomic profile is  \n",
    "    2. target one hot : labels (the correct MoA) '''\n",
    "    \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(df[\"moa\"].unique().reshape(-1,1))\n",
    "    one_hot_encoded = enc.transform(enc.categories_[0].reshape(-1,1)).toarray()\n",
    "    dicti = {}\n",
    "    for i in range(0, len(enc.categories_[0])):\n",
    "        dicti[str(enc.categories_[0][i])] = one_hot_encoded[i]\n",
    "    return dicti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_dict = dict_splitting_into_tensor(L1000_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_dict['cyclooxygenase inhibitor'] = 1.0\n",
    "moa_dict['dopamine receptor antagonist'] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cyclooxygenase inhibitor': 1.0,\n",
       " 'dopamine receptor antagonist': 0.0,\n",
       " 'adrenergic receptor antagonist': 0.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moa_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels= df_train_labels.replace(moa_dict)\n",
    "df_val_labels= df_val_labels.replace(moa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "2902    0.0\n",
       "2903    1.0\n",
       "2904    1.0\n",
       "2905    1.0\n",
       "2906    0.0\n",
       "Name: moa, Length: 2907, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(dual=False, random_state=123)\n",
    "log_reg = LogisticRegression(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set with UMAP transformation: 0.504\n"
     ]
    }
   ],
   "source": [
    "# Transformation with UMAP followed by classification with a linear SVM\n",
    "umap = UMAP(random_state=456)\n",
    "pipeline = Pipeline([(\"umap\", umap), (\"logreg\", log_reg)])\n",
    "params_grid_pipeline = {\n",
    "    \"umap__n_neighbors\": [5, 20]\n",
    "    #\"umap__n_components\": [15, 25, 50]\n",
    "    #\"svc__C\": [10 ** k for k in range(-3, 4)],\n",
    "}\n",
    "\n",
    "\n",
    "clf_pipeline = GridSearchCV(pipeline, params_grid_pipeline)\n",
    "clf_pipeline.fit(df_train_features.values, df_train_labels.values)\n",
    "print(\"Accuracy on the test set with UMAP transformation: {:.3f}\".format(\n",
    "        clf_pipeline.score(df_val_features.values, df_val_labels.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'umap__n_neighbors': 20}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipeline.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run = neptune.init_run(project='erik-everett-palm/Tomics-PCA-UMAP')\n",
    "run[\"parameters/moa_dictionary\"] = str(moa_dictionary)\n",
    "run[\"parameters/train_filename\"] = train_filename\n",
    "norm = False\n",
    "if norm:\n",
    "    scaler = StandardScaler()\n",
    "    df_train[df_train.columns[:-1]] = StandardScaler().fit_transform(df_train[df_train.columns[:-1]])\n",
    "    run[\"parameters/normalize\"] = str(scaler)\n",
    "else:\n",
    "    run[\"parameters/normalize\"] = \"None\"\n",
    "    \n",
    "df_train = pd.merge(df_train, L1000_training[[\"sig_id\", \"moa\"]], how = \"outer\", on =\"sig_id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_train.drop(columns = [\"sig_id\"], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Optuna for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    solver = trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2'])\n",
    "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "    classifier = LogisticRegression(solver=solver, penalty=penalty, class_weight=class_weight)\n",
    "    classifier.fit(df_train_features.values, df_train_labels.values)\n",
    "    predictions = classifier.predict(df_val_features.values)\n",
    "    return f1_score(df_val_labels, predictions, average= \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation with UMAP followed by classification with a linear SVM\n",
    "umap = UMAP(random_state=456)\n",
    "pipeline = Pipeline([(\"umap\", umap), (\"logreg\", log_reg)])\n",
    "params_grid_pipeline = {\n",
    "    \"umap__n_neighbors\": [5, 20]\n",
    "    #\"umap__n_components\": [15, 25, 50]\n",
    "    #\"svc__C\": [10 ** k for k in range(-3, 4)],\n",
    "}\n",
    "\n",
    "\n",
    "clf_pipeline = GridSearchCV(pipeline, params_grid_pipeline)\n",
    "clf_pipeline.fit(df_train_features.values, df_train_labels.values)\n",
    "print(\"Accuracy on the test set with UMAP transformation: {:.3f}\".format(\n",
    "        clf_pipeline.score(df_val_features.values, df_val_labels.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:33:48,198]\u001b[0m A new study created in memory with name: no-name-cba9d3ee-7324-452b-ab92-41c5b67a3a81\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:33:50,074]\u001b[0m Trial 0 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7264626442118158.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:33:52,974]\u001b[0m Trial 1 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7264626442118158.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:33:56,367]\u001b[0m Trial 2 finished with value: 0.7581918263488603 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': None}. Best is trial 2 with value: 0.7581918263488603.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:00,499]\u001b[0m Trial 3 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:03,666]\u001b[0m Trial 4 finished with value: 0.7619863013698631 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': None}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:34:08,490]\u001b[0m Trial 5 finished with value: 0.7146266431721517 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': None}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:34:10,264]\u001b[0m Trial 6 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:34:13,307]\u001b[0m Trial 7 finished with value: 0.7188575529711125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': None}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:34:19,066]\u001b[0m Trial 8 finished with value: 0.7146266431721517 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': None}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:23,391]\u001b[0m Trial 9 finished with value: 0.7983001658374793 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:27,708]\u001b[0m Trial 10 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 3 with value: 0.8001263055623026.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:31,938]\u001b[0m Trial 11 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 11 with value: 0.8019019485903816.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:36,268]\u001b[0m Trial 12 finished with value: 0.8019507013976142 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 12 with value: 0.8019507013976142.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:40,630]\u001b[0m Trial 13 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 12 with value: 0.8019507013976142.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:45,067]\u001b[0m Trial 14 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 12 with value: 0.8019507013976142.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:34:48,224]\u001b[0m Trial 15 finished with value: 0.7302473698434768 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 12 with value: 0.8019507013976142.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:52,576]\u001b[0m Trial 16 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 12 with value: 0.8019507013976142.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:34:56,999]\u001b[0m Trial 17 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 12 with value: 0.8019507013976142.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:01,465]\u001b[0m Trial 18 finished with value: 0.803727633390009 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:04,775]\u001b[0m Trial 19 finished with value: 0.7949401488191523 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:35:07,958]\u001b[0m Trial 20 finished with value: 0.7302473698434768 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:12,049]\u001b[0m Trial 21 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:16,296]\u001b[0m Trial 22 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:20,676]\u001b[0m Trial 23 finished with value: 0.7965249777345964 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:24,969]\u001b[0m Trial 24 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:35:34,081]\u001b[0m Trial 25 finished with value: 0.7186666536087254 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:38,419]\u001b[0m Trial 26 finished with value: 0.7763272297849699 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:42,785]\u001b[0m Trial 27 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:47,068]\u001b[0m Trial 28 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:35:48,981]\u001b[0m Trial 29 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:35:54,971]\u001b[0m Trial 30 finished with value: 0.7186666536087254 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:35:59,099]\u001b[0m Trial 31 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:03,323]\u001b[0m Trial 32 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:36:04,991]\u001b[0m Trial 33 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:08,419]\u001b[0m Trial 34 finished with value: 0.7967303704135082 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:12,809]\u001b[0m Trial 35 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:17,082]\u001b[0m Trial 36 finished with value: 0.7817804375679138 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:21,167]\u001b[0m Trial 37 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:24,396]\u001b[0m Trial 38 finished with value: 0.7600454287336741 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:36:28,381]\u001b[0m Trial 39 finished with value: 0.7302473698434768 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:36:30,678]\u001b[0m Trial 40 finished with value: 0.7169486215538847 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:34,752]\u001b[0m Trial 41 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:38,798]\u001b[0m Trial 42 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:42,783]\u001b[0m Trial 43 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:46,854]\u001b[0m Trial 44 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:50,809]\u001b[0m Trial 45 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:36:54,168]\u001b[0m Trial 46 finished with value: 0.7186666536087254 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:36:58,193]\u001b[0m Trial 47 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:37:01,326]\u001b[0m Trial 48 finished with value: 0.7302473698434768 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:05,313]\u001b[0m Trial 49 finished with value: 0.7780871258213677 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:09,395]\u001b[0m Trial 50 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:13,424]\u001b[0m Trial 51 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:17,471]\u001b[0m Trial 52 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:21,660]\u001b[0m Trial 53 finished with value: 0.803727633390009 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:24,781]\u001b[0m Trial 54 finished with value: 0.7967303704135082 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:29,566]\u001b[0m Trial 55 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:34,168]\u001b[0m Trial 56 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:38,474]\u001b[0m Trial 57 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:37:39,480]\u001b[0m Trial 58 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:37:51,067]\u001b[0m Trial 59 finished with value: 0.7186666536087254 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:37:54,069]\u001b[0m Trial 60 finished with value: 0.7188575529711125 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:37:58,441]\u001b[0m Trial 61 finished with value: 0.8000745071996631 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:02,784]\u001b[0m Trial 62 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:07,138]\u001b[0m Trial 63 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:11,272]\u001b[0m Trial 64 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:15,543]\u001b[0m Trial 65 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:19,884]\u001b[0m Trial 66 finished with value: 0.7983001658374793 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:24,257]\u001b[0m Trial 67 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:27,598]\u001b[0m Trial 68 finished with value: 0.8003073886525052 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:31,950]\u001b[0m Trial 69 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:36,221]\u001b[0m Trial 70 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:40,579]\u001b[0m Trial 71 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:44,795]\u001b[0m Trial 72 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:49,158]\u001b[0m Trial 73 finished with value: 0.803727633390009 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:38:53,463]\u001b[0m Trial 74 finished with value: 0.8001729011452274 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:38:54,883]\u001b[0m Trial 75 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:39:02,664]\u001b[0m Trial 76 finished with value: 0.7146266431721517 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:07,295]\u001b[0m Trial 77 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:11,578]\u001b[0m Trial 78 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:39:14,742]\u001b[0m Trial 79 finished with value: 0.7302473698434768 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:19,093]\u001b[0m Trial 80 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:23,400]\u001b[0m Trial 81 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:27,797]\u001b[0m Trial 82 finished with value: 0.8019507013976142 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:32,141]\u001b[0m Trial 83 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:36,450]\u001b[0m Trial 84 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:40,665]\u001b[0m Trial 85 finished with value: 0.803727633390009 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:44,646]\u001b[0m Trial 86 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:48,717]\u001b[0m Trial 87 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:52,904]\u001b[0m Trial 88 finished with value: 0.7763272297849699 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': None}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:39:56,072]\u001b[0m Trial 89 finished with value: 0.7967303704135082 and parameters: {'solver': 'sag', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:40:00,159]\u001b[0m Trial 90 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:40:04,202]\u001b[0m Trial 91 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:40:08,185]\u001b[0m Trial 92 finished with value: 0.7983498050593889 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:40:12,292]\u001b[0m Trial 93 finished with value: 0.8001263055623026 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:40:16,342]\u001b[0m Trial 94 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-02-17 16:40:17,180]\u001b[0m Trial 95 finished with value: 0.7264626442118158 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-02-17 16:40:20,578]\u001b[0m Trial 96 finished with value: 0.7186666536087254 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:40:24,560]\u001b[0m Trial 97 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-02-17 16:40:28,549]\u001b[0m Trial 98 finished with value: 0.8019019485903816 and parameters: {'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:40:31,627]\u001b[0m Trial 99 finished with value: 0.7302473698434768 and parameters: {'solver': 'liblinear', 'penalty': 'l2', 'class_weight': 'balanced'}. Best is trial 18 with value: 0.803727633390009.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'saga', 'penalty': 'l2', 'class_weight': 'balanced'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "def objective2(trial):\n",
    "    alphas = trial.suggest_float('alpha', 0.1, 15)\n",
    "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "    classifier = RidgeClassifierCV(alphas = alphas, class_weight= class_weight)\n",
    "    classifier.fit(df_train_features.values, df_train_labels.values)\n",
    "    predictions = classifier.predict(df_val_features.values)\n",
    "    return f1_score(df_val_labels, predictions, average= \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-17 16:49:52,181]\u001b[0m A new study created in memory with name: no-name-39ccf96e-24a8-48d0-b3d7-a53678297607\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:50:05,981]\u001b[0m Trial 0 finished with value: 0.7226627280265341 and parameters: {'alpha': 14.9246610880335, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:50:23,375]\u001b[0m Trial 1 finished with value: 0.691077990817729 and parameters: {'alpha': 1.910641794240239, 'class_weight': None}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:50:43,782]\u001b[0m Trial 2 finished with value: 0.6873104367472327 and parameters: {'alpha': 8.564382170097081, 'class_weight': None}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:50:54,675]\u001b[0m Trial 3 finished with value: 0.6891956891956892 and parameters: {'alpha': 6.993518117301544, 'class_weight': None}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:51:08,983]\u001b[0m Trial 4 finished with value: 0.7172957655250587 and parameters: {'alpha': 4.129598364369725, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:51:20,483]\u001b[0m Trial 5 finished with value: 0.7208247623058359 and parameters: {'alpha': 9.213366450955938, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:51:32,275]\u001b[0m Trial 6 finished with value: 0.6873104367472327 and parameters: {'alpha': 10.856040805106307, 'class_weight': None}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:51:46,376]\u001b[0m Trial 7 finished with value: 0.6891956891956892 and parameters: {'alpha': 6.112582613030178, 'class_weight': None}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:52:04,078]\u001b[0m Trial 8 finished with value: 0.7226627280265341 and parameters: {'alpha': 13.795897929416219, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:52:18,279]\u001b[0m Trial 9 finished with value: 0.691077990817729 and parameters: {'alpha': 13.410536271404947, 'class_weight': None}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:52:44,373]\u001b[0m Trial 10 finished with value: 0.7226627280265341 and parameters: {'alpha': 14.863802547520358, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:52:57,173]\u001b[0m Trial 11 finished with value: 0.7226627280265341 and parameters: {'alpha': 12.560222080675437, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:53:15,467]\u001b[0m Trial 12 finished with value: 0.7226627280265341 and parameters: {'alpha': 14.916000767898863, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:53:36,387]\u001b[0m Trial 13 finished with value: 0.7226627280265341 and parameters: {'alpha': 11.663692472631311, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:53:53,290]\u001b[0m Trial 14 finished with value: 0.7226627280265341 and parameters: {'alpha': 10.579387982293278, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:54:21,168]\u001b[0m Trial 15 finished with value: 0.7226627280265341 and parameters: {'alpha': 13.436294686476558, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:54:34,779]\u001b[0m Trial 16 finished with value: 0.7226627280265341 and parameters: {'alpha': 12.545264221413039, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:54:55,268]\u001b[0m Trial 17 finished with value: 0.7226627280265341 and parameters: {'alpha': 14.874110357713576, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:55:08,585]\u001b[0m Trial 18 finished with value: 0.7208247623058359 and parameters: {'alpha': 9.830073510661357, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:55:30,864]\u001b[0m Trial 19 finished with value: 0.7226627280265341 and parameters: {'alpha': 11.37598988134406, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:55:42,078]\u001b[0m Trial 20 finished with value: 0.7208247623058359 and parameters: {'alpha': 8.69414664826904, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:55:53,876]\u001b[0m Trial 21 finished with value: 0.7226627280265341 and parameters: {'alpha': 14.910310405948575, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:56:09,175]\u001b[0m Trial 22 finished with value: 0.7226627280265341 and parameters: {'alpha': 13.716400218357839, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:56:24,179]\u001b[0m Trial 23 finished with value: 0.7226627280265341 and parameters: {'alpha': 12.499529688988416, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n",
      "\u001b[32m[I 2023-02-17 16:56:37,486]\u001b[0m Trial 24 finished with value: 0.7226627280265341 and parameters: {'alpha': 13.870129896587756, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7226627280265341.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study2 = optuna.create_study(direction='maximize')\n",
    "study2.optimize(objective2, n_trials=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import LinearDiscriminantAnalysis\n",
    "def objective3(trial):\n",
    "    solver = trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2'])\n",
    "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
    "    classifier = LogisticRegression(solver=solver, penalty=penalty, class_weight=class_weight)\n",
    "    classifier.fit(df_train_features.values, df_train_labels.values)\n",
    "    predictions = classifier.predict(df_val_features.values)\n",
    "    return f1_score(df_val_labels, predictions, average= \"macro\")\n",
    "\n",
    "study3 = optuna.create_study(direction='maximize')\n",
    "study3.optimize(objective3, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
