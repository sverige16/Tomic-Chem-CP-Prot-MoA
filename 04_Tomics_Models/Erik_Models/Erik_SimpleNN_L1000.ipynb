{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split # Functipn to split data into training, validation and test sets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "import glob   # The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order. No tilde expansion is done, but *, ?, and character ranges expressed with [] will be correctly matched.\n",
    "import os   # miscellneous operating system interfaces. This module provides a portable way of using operating system dependent functionality. If you just want to read or write a file see open(), if you want to manipulate paths, see the os.path module, and if you want to read all the lines in all the files on the command line see the fileinput module.\n",
    "import random       \n",
    "from tqdm import tqdm \n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import datetime\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchinfo\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve,log_loss\n",
    "from sklearn.metrics import average_precision_score,roc_auc_score\n",
    "import os\n",
    "import time\n",
    "from time import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from skmultilearn.adapt import MLkNN\n",
    "\n",
    "# CMAP (extracting relevant transcriptomic profiles)\n",
    "from cmapPy.pandasGEXpress.parse import parse\n",
    "import cmapPy.pandasGEXpress.subset_gctoo as sg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_battery_L1000 import tprofiles_gc_too_func, extract_tprofile, load_train_valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_cuda = True\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading all relevant data frames and csv files ----------------------------------------------------------\n",
    "\n",
    "# clue column metadata with columns representing compounds in common with SPECs 1 & 2\n",
    "clue_sig_in_SPECS = pd.read_csv('/home/jovyan/Tomics-CP-Chem-MoA/04_Tomics_Models/init_data_expl/clue_sig_in_SPECS1&2.csv', delimiter = \",\")\n",
    "\n",
    "# clue row metadata with rows representing transcription levels of specific genes\n",
    "clue_gene = pd.read_csv('/home/jovyan/Tomics-CP-Chem-MoA/04_Tomics_Models/init_data_expl/clue_geneinfo_beta.txt', delimiter = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_into_tensor(df, num_classes):\n",
    "    '''Splitting data into two parts:\n",
    "    1. input : the pointer showing where the transcriptomic profile is  \n",
    "    2. target one hot : labels (the correct MoA) '''\n",
    "    \n",
    "    # one-hot encoding labels\n",
    "     # creating tensor from all_data.df\n",
    "    target = torch.tensor(df['moa'].values.astype(np.int64))\n",
    "\n",
    "    # For each row, take the index of the target label\n",
    "    # (which coincides with the score in our case) and use it as the column index to set the value 1.0.‚Äù \n",
    "    target_onehot = torch.zeros(target.shape[0], num_classes)\n",
    "    target_onehot.scatter_(1, target.unsqueeze(1), 1.0)\n",
    "    \n",
    "    input =  df.drop('moa', axis = 1)\n",
    "    \n",
    "    return input, target_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcriptomic_Profiles(torch.utils.data.Dataset):\n",
    "    def __init__(self, labels, gc_too):\n",
    "        self.tprofile_labels = labels\n",
    "        self.profiles_gc_too = gc_too\n",
    "        \n",
    "    def __len__(self):\n",
    "        ''' The number of data points '''\n",
    "        return len(self.tprofile_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Retreiving the transcriptomic profile and label'''\n",
    "        t_profile = extract_tprofile(self.profiles_gc_too, idx)          # extract image from csv using index\n",
    "        t_profile = torch.tensor(t_profile)       # turn t profile into a floating torch tensor\n",
    "        label = self.tprofile_labels[idx]          # extract calssification using index\n",
    "        return torch.squeeze(t_profile), torch.squeeze(label) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda. \n"
     ]
    }
   ],
   "source": [
    "batch_size = 10 \n",
    "# parameters\n",
    "params = {'batch_size' : batch_size,\n",
    "         'num_workers' : 3,\n",
    "         'shuffle' : True,\n",
    "         'prefetch_factor' : 2} \n",
    "          \n",
    "if using_cuda:\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Training on device {device}. ' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'L1000_training_set_train_2APC1.csv'\n",
    "valid_filename = 'L1000_test_set_train_2APC1.csv'\n",
    "L1000_training, L1000_validation = load_train_valid_data(train_filename, valid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling training and validation data \n",
    "# May not be necessary given params\n",
    "L1000_training = L1000_training.sample(frac = 1, random_state = 1)\n",
    "L1000_validation = L1000_validation.sample(frac = 1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_gc_too_train = tprofiles_gc_too_func(L1000_training, clue_gene)\n",
    "profiles_gc_too_valid = tprofiles_gc_too_func(L1000_validation, clue_gene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Compound ID', 'sig_id', 'moa'], dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1000_training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(L1000_training[\"moa\"].unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "input_df_val, labels_train = splitting_into_tensor(L1000_training, num_classes) \n",
    "input_df_val, labels_val = splitting_into_tensor(L1000_validation, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator: training\n",
    "# create a subset with only train indices\n",
    "\n",
    "# create generator that randomly takes indices from the training set\n",
    "training_dataset = Transcriptomic_Profiles(labels_train, profiles_gc_too_train)\n",
    "\n",
    "\n",
    "\n",
    "training_generator = torch.utils.data.DataLoader(training_dataset, **params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator: validation\n",
    "# create a subset with only valid indices\n",
    "\n",
    "# create generator that randomly takes indices from the validation set\n",
    "validation_dataset = Transcriptomic_Profiles(labels_val, profiles_gc_too_valid)\n",
    "\n",
    "\n",
    "\n",
    "validation_generator = torch.utils.data.DataLoader(validation_dataset, **params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_profile, train_labels = next(iter(training_generator))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6929,  0.3499, -0.7044,  ..., -0.1561, -2.6069,  0.4294],\n",
       "        [ 0.9971,  1.3566, -0.4538,  ..., -1.1392, -0.9483, -0.3819],\n",
       "        [ 0.8089, -1.1247,  0.2660,  ...,  0.0076,  0.2925, -0.1902],\n",
       "        ...,\n",
       "        [-0.0511, -0.3675, -0.4809,  ...,  0.0837, -1.2937,  0.0356],\n",
       "        [ 0.4881, -0.4957, -0.4946,  ..., -0.4459, -2.5714, -0.3795],\n",
       "        [-0.4169,  0.1680,  0.7089,  ..., -0.1847, -0.3225, -0.2401]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile: tensor([[ 0.6929,  0.3499, -0.7044,  ..., -0.1561, -2.6069,  0.4294],\n",
      "        [ 0.9971,  1.3566, -0.4538,  ..., -1.1392, -0.9483, -0.3819],\n",
      "        [ 0.8089, -1.1247,  0.2660,  ...,  0.0076,  0.2925, -0.1902],\n",
      "        ...,\n",
      "        [-0.0511, -0.3675, -0.4809,  ...,  0.0837, -1.2937,  0.0356],\n",
      "        [ 0.4881, -0.4957, -0.4946,  ..., -0.4459, -2.5714, -0.3795],\n",
      "        [-0.4169,  0.1680,  0.7089,  ..., -0.1847, -0.3225, -0.2401]]), train label: tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(f'profile: {train_profile}, train label: {train_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_profile.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_profile, valid_labels = next(iter(validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile: tensor([[ 1.6750e-04, -5.0194e-01,  4.6463e-01,  ...,  5.1980e-01,\n",
      "          1.0402e+00, -1.8111e-02],\n",
      "        [-1.6815e+00,  3.8334e+00,  2.7112e+00,  ...,  4.7289e+00,\n",
      "         -6.6505e-01, -1.4110e+00],\n",
      "        [-2.2784e-01,  3.9844e-01, -2.8441e-01,  ..., -5.4222e-01,\n",
      "         -3.2291e-01,  8.3311e-01],\n",
      "        ...,\n",
      "        [ 3.9535e-01,  6.4416e-01,  9.3608e-01,  ...,  3.4359e-01,\n",
      "         -3.9971e-01,  2.5293e-01],\n",
      "        [ 1.8392e-01,  8.2916e-02, -3.0253e-01,  ...,  1.2666e-02,\n",
      "         -6.8398e-01, -3.8505e-01],\n",
      "        [-8.3986e-01,  1.1738e+00, -1.8197e-01,  ...,  6.9134e-01,\n",
      "          1.1246e+00,  2.7283e-01]]), train label: tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(f'profile: {valid_profile}, train label: {valid_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 3-Layer FeedForward Neural Network\n",
    "    \n",
    "    For more info: https://github.com/guitarmind/kaggle_moa_winner_hungry_for_gold\\\n",
    "    /blob/main/final/Best%20LB/Training/3-stagenn-train.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features = None, num_targets = None, hidden_size = None):\n",
    "        super(SimpleNN_Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.dense2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN_Model(num_features = train_profile.shape[1], num_targets= num_classes, hidden_size= hidden_size)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------- hyperparameters ---------------------------------------#\n",
    "# Hyperparameters\n",
    "testing = False # decides if we take a subset of the data\n",
    "max_epochs = 4 # number of epochs we are going to run \n",
    "# apply_class_weights = True # weight the classes based on number of compounds\n",
    "using_cuda = True # to use available GPUs\n",
    "world_size = torch.cuda.device_count()\n",
    "\n",
    "#----------------------------------------- pre-processing -----------------------------------------#\n",
    "start = time.time()\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%d_%m_%Y-%H:%M:%S\")\n",
    "print(\"Begin Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------Function to perform training, validation, testing, and assessment ------------------\n",
    "\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, valid_loader):\n",
    "    '''\n",
    "    n_epochs: number of epochs \n",
    "    optimizer: optimizer used to do backpropagation\n",
    "    model: deep learning architecture\n",
    "    loss_fn: loss function\n",
    "    train_loader: generator creating batches of training data\n",
    "    valid_loader: generator creating batches of validation data\n",
    "    '''\n",
    "    # lists keep track of loss and accuracy for training and validation set\n",
    "    model = model.to(device)\n",
    "    train_loss_per_epoch = []\n",
    "    train_acc_per_epoch = []\n",
    "    val_loss_per_epoch = []\n",
    "    val_acc_per_epoch = []\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in tqdm(range(1, max_epochs +1), desc = \"Epoch\", position=0, leave= True):\n",
    "        loss_train = 0.0\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        for tprofiles, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # put model, images, labels on the same device\n",
    "            tprofiles = tprofiles.to(device = device)\n",
    "            labels = labels.to(device= device)\n",
    "            # Training Model\n",
    "            outputs = model(tprofiles)\n",
    "            #print(f' Outputs : {outputs}') # tensor with 10 elements\n",
    "            #print(f' Labels : {labels}') # tensor that is a number\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            # For L2 regularization\n",
    "            #l2_lambda = 0.000001\n",
    "            #l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            #loss = loss + l2_lambda * l2_norm\n",
    "            # Update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Training Metrics\n",
    "            loss_train += loss.item()\n",
    "            #print(f' loss: {loss.item()}')\n",
    "            train_predicted = torch.argmax(outputs, 1)\n",
    "            #print(f' train_predicted {train_predicted}')\n",
    "            # NEW\n",
    "            labels = torch.argmax(labels,1)\n",
    "            #print(labels)\n",
    "            train_total += labels.shape[0]\n",
    "            train_correct += int((train_predicted == labels).sum())\n",
    "        # validation metrics from batch\n",
    "        val_correct, val_total, val_loss, best_val_loss_upd = validation_loop(model, loss_fn, valid_loader, best_val_loss)\n",
    "        best_val_loss = best_val_loss_upd\n",
    "        val_accuracy = val_correct/val_total\n",
    "        # printing results for epoch\n",
    "        if epoch == 1 or epoch %5 == 0:\n",
    "            print(f' {datetime.datetime.now()} Epoch: {epoch}, Training loss: {loss_train/len(train_loader)}, Validation Loss: {val_loss} ')\n",
    "        # adding epoch loss, accuracy to lists \n",
    "        val_loss_per_epoch.append(val_loss)\n",
    "        train_loss_per_epoch.append(loss_train/len(train_loader))\n",
    "        val_acc_per_epoch.append(val_accuracy)\n",
    "        train_acc_per_epoch.append(train_correct/train_total)\n",
    "    # return lists with loss, accuracy every epoch\n",
    "    return train_loss_per_epoch, train_acc_per_epoch, val_loss_per_epoch, val_acc_per_epoch\n",
    "                                \n",
    "\n",
    "def validation_loop(model, loss_fn, valid_loader, best_val_loss):\n",
    "    '''\n",
    "    Assessing trained model on valiidation dataset \n",
    "    model: deep learning architecture getting updated by model\n",
    "    loss_fn: loss function\n",
    "    valid_loader: generator creating batches of validation data\n",
    "    '''\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # does not keep track of gradients so as to not train on validation data.\n",
    "        for imgs, labels in valid_loader:\n",
    "            # Move to device MAY NOT BE NECESSARY\n",
    "            imgs = imgs.to(device = device)\n",
    "            labels = labels.to(device= device)\n",
    "            # Assessing outputs\n",
    "            outputs = model(imgs)\n",
    "            # print(f' Outputs : {outputs}') # tensor with 10 elements\n",
    "            # print(f' Labels : {labels}') # tensor that is a number\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss_val += loss.item()\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            labels = torch.argmax(labels,1)\n",
    "            #print(predicted)\n",
    "            #print(labels)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "        avg_val_loss = loss_val/len(valid_loader)  # average loss over batch\n",
    "        if best_val_loss > loss_val:\n",
    "            best_val_loss = loss_val\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict' : model.state_dict(),\n",
    "                    'valid_loss' : loss_val\n",
    "            },  '/home/jovyan/Tomics-CP-Chem-MoA/02_CP_Models/saved_models' +'/' + 'CP_least_loss_model'\n",
    "            )\n",
    "    model.train()\n",
    "    return correct, total, avg_val_loss, best_val_loss\n",
    "\n",
    "\n",
    "def test_loop(model, loss_fn, test_loader):\n",
    "    '''\n",
    "    Assessing trained model on test dataset \n",
    "    model: deep learning architecture getting updated by model\n",
    "    loss_fn: loss function\n",
    "    test_loader: generator creating batches of test data\n",
    "    '''\n",
    "    model.eval()\n",
    "    loss_test = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # does not keep track of gradients so as to not train on test data.\n",
    "        for compounds, labels in tqdm(test_loader,\n",
    "                                            desc = \"Test Batches w/in Epoch\",\n",
    "                                              position = 0,\n",
    "                                              leave = True):\n",
    "            # Move to device MAY NOT BE NECESSARY\n",
    "            model = model.to(device)\n",
    "            compounds = compounds.to(device = device)\n",
    "            labels = labels.to(device= device)\n",
    "            # Assessing outputs\n",
    "            outputs = model(compounds)\n",
    "            # print(f' Outputs : {outputs}') # tensor with 10 elements\n",
    "            # print(f' Labels : {labels}') # tensor that is a number\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            loss_test += loss.item()\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            labels = torch.argmax(labels,1)\n",
    "            #print(predicted)\n",
    "            #print(labels)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "            #print(f' Predicted: {predicted.tolist()}')\n",
    "            #print(f' Labels: {predicted.tolist()}')\n",
    "            all_predictions = all_predictions + predicted.tolist()\n",
    "            all_labels = all_labels + labels.tolist()\n",
    "        results_assessment(all_predictions, all_labels)\n",
    "        avg_test_loss = loss_test/len(test_loader)  # average loss over batch\n",
    "    return correct, total, avg_test_loss\n",
    "\n",
    "#---------------------------------------- Visual Assessment ---------------------------------# \n",
    "\n",
    "def val_vs_train_loss(epochs, train_loss, val_loss):\n",
    "    ''' \n",
    "    Plotting validation versus training loss over time\n",
    "    epochs: number of epochs that the model ran (int. hyperparameter)\n",
    "    train_loss: training loss per epoch (python list)\n",
    "    val_loss: validation loss per epoch (python list)\n",
    "    ''' \n",
    "    loss_path_to_save = '/home/jovyan/Tomics-CP-Chem-MoA/02_CP_Models/saved_images'\n",
    "    plt.figure()\n",
    "    x_axis = list(range(1, epochs +1)) # create x axis with number of\n",
    "    plt.plot(x_axis, train_loss, label = \"train_loss\")\n",
    "    plt.plot(x_axis, val_loss, label = \"val_loss\")\n",
    "    # Figure description\n",
    "    plt.xlabel('# of Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation versus Training Loss: CP Image Model')\n",
    "    plt.legend()\n",
    "    # plot\n",
    "    plt.savefig(loss_path_to_save + '/' + 'loss_train_val_' + now)\n",
    "\n",
    "\n",
    "def val_vs_train_accuracy(epochs, train_acc, val_acc):\n",
    "    '''\n",
    "    Plotting validation versus training loss over time\n",
    "    epochs: number of epochs that the model ran (int. hyperparameter)\n",
    "    train_acc: accuracy loss per epoch (python list)\n",
    "    val_acc: accuracy loss per epoch (python list)\n",
    "    '''\n",
    "    acc_path_to_save = '/home/jovyan/Tomics-CP-Chem-MoA/02_CP_Models/saved_images'\n",
    "    plt.figure()\n",
    "    x_axis = list(range(1, epochs +1)) # create x axis with number of\n",
    "    plt.plot(x_axis, train_acc, label = \"train_acc\")\n",
    "    plt.plot(x_axis, val_acc, label = \"val_acc\")\n",
    "    # Figure description\n",
    "    plt.xlabel('# of Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation versus Training Accuracy: CP Image Model')\n",
    "    plt.legend()\n",
    "    # plot\n",
    "    plt.savefig(acc_path_to_save + '/' + 'acc_train_val_' + now)\n",
    "\n",
    "#------------------------------   Calling functions --------------------------- #\n",
    "train_loss_per_epoch, train_acc_per_epoch, val_loss_per_epoch, val_acc_per_epoch = training_loop(n_epochs = max_epochs,\n",
    "              optimizer = cnn_optimizer,\n",
    "              model = updated_model,\n",
    "              loss_fn = loss_function,\n",
    "              train_loader=training_generator, \n",
    "              valid_loader=valid_generator)\n",
    "\n",
    "\n",
    "\n",
    "val_vs_train_loss(max_epochs,train_loss_per_epoch, val_loss_per_epoch)\n",
    "\n",
    "\n",
    "val_vs_train_accuracy(max_epochs, train_acc_per_epoch, val_acc_per_epoch)\n",
    "\n",
    "correct, total, avg_test_loss = test_loop(model = updated_model,\n",
    "                                          loss_fn = loss_function, \n",
    "                                          test_loader = test_generator)\n",
    "\n",
    "#-------------------------------- Writing interesting info into terminal ------------------------# \n",
    "\n",
    "end = time.time()\n",
    "def program_elapsed_time(start, end):\n",
    "    program_time = round(end - start, 2) \n",
    "    print(program_time)\n",
    "    if program_time > float(60) and program_time < 60*60:\n",
    "        program_time =  program_time/60\n",
    "        time_elapsed = str(program_time) + ' min'\n",
    "    elif program_time > 60*60:\n",
    "        program_time = program_time/3600\n",
    "        time_elapsed = str(program_time) + ' hrs'\n",
    "    else:\n",
    "        time_elapsed = str(program_time) + ' sec'\n",
    "    return time_elapsed\n",
    "program_elapsed_time = program_elapsed_time(start, end)\n",
    "\n",
    "test_set_acc = f' {round(correct/total*100, 2)} %'\n",
    "table = [[\"Time to Run Program\", program_elapsed_time],\n",
    "['Accuracy of Test Set', test_set_acc]]\n",
    "print(tabulate(table, tablefmt='fancy_grid'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
